{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_TC1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOikwwrKwex5V08T1m54lov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengyaoo/FNL_GenesSelection/blob/main/04_TC1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOdSOBujy5SZ"
      },
      "source": [
        "# Load the libraries\n",
        "from __future__ import print_function\n",
        "\n",
        "import os, sys, gzip, glob, json, time, argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from pandas.io.json import json_normalize\n",
        "\n",
        "from pandas.io.json import json_normalize\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Conv1D, MaxPooling1D, Flatten\n",
        "from keras import optimizers\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.models import Sequential, Model, model_from_json, model_from_yaml\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.linear_model import Lasso, LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import sklearn.manifold as sk_manif\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8upmBOzzBiB",
        "outputId": "48de68ad-8d45-4f19-c18b-8e6e4ccf6194"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8im6yPQlzGRz"
      },
      "source": [
        "# Read features and output files \n",
        "TC1data15 = pd.read_csv(\"/content/drive/My Drive/FNL_TC1/TC1-S1-data15-genename.tsv\", sep=\"\\t\", low_memory = False)\n",
        "outcome = pd.read_csv('/content/drive/My Drive/FNL_TC1/TC1-outcome-data15-projectname.tsv', sep='\\t')\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhYQ0BrDzH6N"
      },
      "source": [
        "def encode(data): \n",
        "    print('Shape of data (BEFORE encode): %s' % str(data.shape))\n",
        "    encoded = to_categorical(data)\n",
        "    print('Shape of data (AFTER  encode): %s\\n' % str(encoded.shape))\n",
        "    return encoded"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_1Vevs_zO5r",
        "outputId": "94edd6c8-7fea-4b8c-fae3-0265507f9cf1"
      },
      "source": [
        "outcome = encode(outcome['Project_id'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data (BEFORE encode): (4500,)\n",
            "Shape of data (AFTER  encode): (4500, 15)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5eU9aOLzufQ"
      },
      "source": [
        "# Train/Test split  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb59wOPrzpnd"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(TC1data15, outcome, \n",
        "                                                    train_size=0.75, \n",
        "                                                    test_size=0.25, \n",
        "                                                    random_state=123, \n",
        "                                                    stratify = outcome)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmcRFK8Ezypu"
      },
      "source": [
        "# CONV1D "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwgw-7fXzqRb",
        "outputId": "2e57c5e0-bc2d-452f-b6c8-dcab85eb84c1"
      },
      "source": [
        "# parameters  \n",
        "activation='relu'\n",
        "batch_size=20\n",
        "# Number of sites\n",
        "classes=15\n",
        "drop = 0.1\n",
        "feature_subsample = 0\n",
        "loss='categorical_crossentropy'\n",
        "# metrics='accuracy'\n",
        "out_act='softmax'\n",
        "pool=[1, 10]\n",
        "# optimizer='sgd'\n",
        "shuffle = False \n",
        "epochs=30\n",
        "\n",
        "optimizer = optimizers.SGD(lr=0.1)\n",
        "metrics = ['acc']\n",
        "\n",
        "# X_train shape: (3375, 60483)\n",
        "# X_test shape:  (1125, 60483)\n",
        "# Y_train shape: (3375,1)\n",
        "# Y_test shape:  (1125,1)\n",
        "\n",
        "# 60483\n",
        "x_train_len = X_train.shape[1]   \n",
        "\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "# X_train shape: (3375, 60483, 1)\n",
        "# X_test shape:  (1125, 60483, 1)\n",
        "\n",
        "\n",
        "filters = 128 \n",
        "filter_len = 20 \n",
        "stride = 1 \n",
        "\n",
        "# inside pool_list loop\n",
        "pool_list = [1,10]\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# model.add  CONV1D\n",
        "model.add(Conv1D(filters = filters, \n",
        "                 kernel_size = filter_len, \n",
        "                 strides = stride, \n",
        "                 padding='valid', \n",
        "                 input_shape=(x_train_len, 1)))\n",
        "\n",
        "# x_train_len = 60,483\n",
        "# Activation\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# MaxPooling\n",
        "model.add(MaxPooling1D(pool_size = 1))\n",
        "\n",
        "filters = 128\n",
        "filter_len = 10 \n",
        "stride = 1 \n",
        "# Conv1D\n",
        "model.add(Conv1D(filters=filters, \n",
        "                 kernel_size=filter_len, \n",
        "                 strides=stride, \n",
        "                 padding='valid'))\n",
        "# Activation\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# MaxPooling\n",
        "model.add(MaxPooling1D(pool_size = 10))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(200))\n",
        "\n",
        "# activation \n",
        "# model.add(Activation('relu')) # SR\n",
        "model.add(Activation(activation))\n",
        "#dropout\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(20))\n",
        "# activation\n",
        "# model.add(Activation('relu')) # SR\n",
        "model.add(Activation(activation))\n",
        "\n",
        "#dropout\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(15))\n",
        "model.add(Activation(out_act))\n",
        "\n",
        "model.compile( loss= loss, \n",
        "              optimizer = optimizer, \n",
        "              metrics = metrics )\n",
        "model.summary()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 60464, 128)        2688      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 60464, 128)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 60464, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 60455, 128)        163968    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 60455, 128)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 6045, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 773760)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               154752200 \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                4020      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 15)                315       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 154,923,191\n",
            "Trainable params: 154,923,191\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VCxYIYozqUq"
      },
      "source": [
        "# save\n",
        "save = '.'\n",
        "output_dir = \"/content/drive/My Drive/FNL_TC1/Model\"\n",
        "          \n",
        "if not os.path.exists(output_dir): \n",
        "\tos.makedirs(output_dir)\n",
        "\n",
        "model_name = 'tc1'\n",
        "path = '{}/{}.autosave.model.h5'.format(output_dir, model_name)\n",
        "checkpointer = ModelCheckpoint(filepath=path, \n",
        "                               verbose=1, \n",
        "                               save_weights_only=False, \n",
        "                               save_best_only=True)\n",
        "          \n",
        "csv_logger = CSVLogger('{}/training.log'.format(output_dir))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wvSm3M5zqXy"
      },
      "source": [
        "# SR: change epsilon to min_delta\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
        "                              factor=0.1, \n",
        "                              patience=10, \n",
        "                              verbose=1, mode='auto', \n",
        "                              min_delta=0.0001, \n",
        "                              cooldown=0, \n",
        "                              min_lr=0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnNqZLiNzqaq",
        "outputId": "53c5922a-f44d-4634-ec5d-47b8aa43f811"
      },
      "source": [
        "# batch_size = 20 \n",
        "history = model.fit(X_train, Y_train, batch_size=batch_size, \n",
        "                    epochs=epochs, verbose=1, validation_data=(X_test, Y_test), \n",
        "                    callbacks = [checkpointer, csv_logger, reduce_lr])\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "169/169 [==============================] - 2377s 14s/step - loss: 2.8863 - acc: 0.0676 - val_loss: 2.6337 - val_acc: 0.2347\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.63367, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "Epoch 2/30\n",
            "169/169 [==============================] - 2427s 14s/step - loss: 2.3059 - acc: 0.2631 - val_loss: 0.7943 - val_acc: 0.7182\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.63367 to 0.79428, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "Epoch 3/30\n",
            "169/169 [==============================] - 2464s 15s/step - loss: 0.8356 - acc: 0.7191 - val_loss: 0.3502 - val_acc: 0.8951\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.79428 to 0.35020, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "Epoch 4/30\n",
            "169/169 [==============================] - 2438s 14s/step - loss: 0.3516 - acc: 0.8806 - val_loss: 0.4165 - val_acc: 0.8524\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.35020\n",
            "Epoch 5/30\n",
            "169/169 [==============================] - 2418s 14s/step - loss: 0.2227 - acc: 0.9325 - val_loss: 0.2947 - val_acc: 0.8862\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.35020 to 0.29469, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "Epoch 6/30\n",
            "169/169 [==============================] - 2399s 14s/step - loss: 0.1752 - acc: 0.9397 - val_loss: 0.2816 - val_acc: 0.9147\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.29469 to 0.28157, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "Epoch 7/30\n",
            "169/169 [==============================] - 2397s 14s/step - loss: 0.1380 - acc: 0.9503 - val_loss: 0.1872 - val_acc: 0.9476\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.28157 to 0.18724, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "Epoch 8/30\n",
            "169/169 [==============================] - 2421s 14s/step - loss: 0.0989 - acc: 0.9683 - val_loss: 0.1669 - val_acc: 0.9564\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.18724 to 0.16692, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "Epoch 9/30\n",
            "169/169 [==============================] - 2442s 14s/step - loss: 0.1043 - acc: 0.9638 - val_loss: 0.1627 - val_acc: 0.9564\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.16692 to 0.16268, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "Epoch 10/30\n",
            "169/169 [==============================] - 2446s 14s/step - loss: 0.0770 - acc: 0.9734 - val_loss: 0.1869 - val_acc: 0.9591\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.16268\n",
            "Epoch 11/30\n",
            "169/169 [==============================] - 2370s 14s/step - loss: 0.0677 - acc: 0.9796 - val_loss: 0.1619 - val_acc: 0.9644\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.16268 to 0.16189, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "Epoch 12/30\n",
            "169/169 [==============================] - 2367s 14s/step - loss: 0.0603 - acc: 0.9797 - val_loss: 0.1779 - val_acc: 0.9600\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.16189\n",
            "Epoch 13/30\n",
            "169/169 [==============================] - 2353s 14s/step - loss: 0.0201 - acc: 0.9930 - val_loss: 0.3856 - val_acc: 0.8862\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.16189\n",
            "Epoch 14/30\n",
            "169/169 [==============================] - 2348s 14s/step - loss: 0.0412 - acc: 0.9853 - val_loss: 0.1784 - val_acc: 0.9653\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.16189\n",
            "Epoch 15/30\n",
            "169/169 [==============================] - 2339s 14s/step - loss: 0.0553 - acc: 0.9827 - val_loss: 0.1944 - val_acc: 0.9636\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.16189\n",
            "Epoch 16/30\n",
            "169/169 [==============================] - 2344s 14s/step - loss: 0.0654 - acc: 0.9835 - val_loss: 0.1540 - val_acc: 0.9636\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.16189 to 0.15399, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "Epoch 17/30\n",
            "169/169 [==============================] - 2348s 14s/step - loss: 0.1017 - acc: 0.9679 - val_loss: 0.4363 - val_acc: 0.9102\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.15399\n",
            "Epoch 18/30\n",
            "169/169 [==============================] - 2347s 14s/step - loss: 0.0948 - acc: 0.9741 - val_loss: 0.2478 - val_acc: 0.9502\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.15399\n",
            "Epoch 19/30\n",
            "169/169 [==============================] - 2343s 14s/step - loss: 0.0472 - acc: 0.9843 - val_loss: 0.1813 - val_acc: 0.9547\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.15399\n",
            "Epoch 20/30\n",
            "169/169 [==============================] - 2348s 14s/step - loss: 0.0428 - acc: 0.9884 - val_loss: 0.1886 - val_acc: 0.9573\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.15399\n",
            "Epoch 21/30\n",
            "169/169 [==============================] - 2346s 14s/step - loss: 0.0355 - acc: 0.9900 - val_loss: 0.2051 - val_acc: 0.9618\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.15399\n",
            "Epoch 22/30\n",
            "169/169 [==============================] - 2348s 14s/step - loss: 0.0525 - acc: 0.9850 - val_loss: 0.2168 - val_acc: 0.9556\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.15399\n",
            "Epoch 23/30\n",
            "169/169 [==============================] - 2357s 14s/step - loss: 0.0341 - acc: 0.9861 - val_loss: 0.1609 - val_acc: 0.9662\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.15399\n",
            "Epoch 24/30\n",
            "169/169 [==============================] - 2346s 14s/step - loss: 0.0243 - acc: 0.9914 - val_loss: 0.1851 - val_acc: 0.9689\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.15399\n",
            "Epoch 25/30\n",
            "169/169 [==============================] - 2348s 14s/step - loss: 0.0127 - acc: 0.9961 - val_loss: 0.2286 - val_acc: 0.9662\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.15399\n",
            "Epoch 26/30\n",
            "169/169 [==============================] - 2338s 14s/step - loss: 0.0230 - acc: 0.9930 - val_loss: 0.1920 - val_acc: 0.9671\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.15399\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
            "Epoch 27/30\n",
            "169/169 [==============================] - 2346s 14s/step - loss: 0.0158 - acc: 0.9944 - val_loss: 0.1933 - val_acc: 0.9636\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.15399\n",
            "Epoch 28/30\n",
            "169/169 [==============================] - 2335s 14s/step - loss: 0.0125 - acc: 0.9958 - val_loss: 0.1890 - val_acc: 0.9671\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.15399\n",
            "Epoch 29/30\n",
            "169/169 [==============================] - 2353s 14s/step - loss: 0.0080 - acc: 0.9980 - val_loss: 0.1947 - val_acc: 0.9671\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.15399\n",
            "Epoch 30/30\n",
            "169/169 [==============================] - 2371s 14s/step - loss: 0.0132 - acc: 0.9972 - val_loss: 0.1906 - val_acc: 0.9698\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.15399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eXltMtSRXPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7f8cfc-feac-4e83-e373-71d4de8edee6"
      },
      "source": [
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.19061551988124847\n",
            "Test accuracy: 0.9697777628898621\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7t8bvBu0GPF"
      },
      "source": [
        "# Finish up save model weights    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BHLMlpezqdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82cafe10-7e91-40a0-e336-3fcec5d48707"
      },
      "source": [
        "# JSON JSON\n",
        "# serialize model to json\n",
        "json_model = model.to_json()\n",
        "\n",
        "# save the model architecture to JSON file\n",
        "with open('{}/{}.model.json'.format(output_dir, model_name), 'w') as json_file:\n",
        "    json_file.write(json_model)\n",
        "\n",
        "\n",
        "# YAML YAML\n",
        "# serialize model to YAML\n",
        "model_yaml = model.to_yaml()\n",
        "\n",
        "# save the model architecture to YAML file\n",
        "with open(\"{}/{}.model.yaml\".format(output_dir, model_name), \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "\n",
        "\n",
        "# WEIGHTS HDF5\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"{}/{}.model.h5\".format(output_dir,model_name))\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpOpDU0lMND8",
        "outputId": "ef3dc716-648a-4571-8b86-e3cfbb86f2df"
      },
      "source": [
        "# Open the handle\n",
        "json_file = open('{}/{}.model.json'.format(output_dir, model_name), 'r')\n",
        "\n",
        "# load json and create model\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model.load_weights('{}/{}.model.h5'.format(output_dir, model_name))\n",
        "print(\"Loaded model from disk\")\n",
        "# loaded_model_json\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTEE7F8azqjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11607e8-1a2e-4eb3-ca7c-74412c54d450"
      },
      "source": [
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='categorical_crossentropy', optimizer='sgd', \n",
        "                     metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.19061551988124847\n",
            "Test accuracy: 0.9697777628898621\n",
            "accuracy: 96.98%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwRb6fXgzqnQ"
      },
      "source": [
        "Y_pred=np.argmax(model.predict(X_test),axis=-1)\n",
        "acc_val=np.mean(Y_pred==Y_test)\n",
        "acc_val"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCGp_09EVWuY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
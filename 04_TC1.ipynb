{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_TC1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMESPvQaLU1h4fV4eArOd7l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengyaoo/FNL_GenesSelection/blob/main/04_TC1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOdSOBujy5SZ"
      },
      "source": [
        "# Load the libraries\n",
        "from __future__ import print_function\n",
        "\n",
        "import os, sys, gzip, glob, json, time, argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from pandas.io.json import json_normalize\n",
        "\n",
        "from pandas.io.json import json_normalize\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Conv1D, MaxPooling1D, Flatten\n",
        "from keras import optimizers\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.models import Sequential, Model, model_from_json, model_from_yaml\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.linear_model import Lasso, LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import sklearn.manifold as sk_manif\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8upmBOzzBiB",
        "outputId": "f6c463d9-b010-4b68-99e5-0d7e256f4336"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8im6yPQlzGRz"
      },
      "source": [
        "# Read features and output files \n",
        "TC1data15 = pd.read_csv(\"/content/drive/My Drive/FNL_TC1/TC1-S1-data15-genename.tsv\", sep=\"\\t\", low_memory = False)\n",
        "outcome = pd.read_csv('/content/drive/My Drive/FNL_TC1/TC1-outcome-data15-projectname.tsv', sep='\\t')\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhYQ0BrDzH6N"
      },
      "source": [
        "def encode(data): \n",
        "    print('Shape of data (BEFORE encode): %s' % str(data.shape))\n",
        "    encoded = to_categorical(data)\n",
        "    print('Shape of data (AFTER  encode): %s\\n' % str(encoded.shape))\n",
        "    return encoded"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_1Vevs_zO5r",
        "outputId": "8e03d6a4-5b8a-4894-b090-f1ab90c10e72"
      },
      "source": [
        "outcome = encode(outcome['Project_id'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data (BEFORE encode): (4500,)\n",
            "Shape of data (AFTER  encode): (4500, 15)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5eU9aOLzufQ"
      },
      "source": [
        "# Train/Test split  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb59wOPrzpnd"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(TC1data15, outcome, \n",
        "                                                    train_size=0.75, \n",
        "                                                    test_size=0.25, \n",
        "                                                    random_state=123, \n",
        "                                                    stratify = outcome)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmcRFK8Ezypu"
      },
      "source": [
        "# CONV1D "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwgw-7fXzqRb",
        "outputId": "b8c177e5-7401-4c0b-932d-2ccd857f7773"
      },
      "source": [
        "# parameters  \n",
        "activation='relu'\n",
        "batch_size=20\n",
        "# Number of sites\n",
        "classes=15\n",
        "drop = 0.1\n",
        "feature_subsample = 0\n",
        "loss='categorical_crossentropy'\n",
        "# metrics='accuracy'\n",
        "out_act='softmax'\n",
        "pool=[1, 10]\n",
        "# optimizer='sgd'\n",
        "shuffle = False \n",
        "epochs=50\n",
        "\n",
        "optimizer = optimizers.SGD(lr=0.1)\n",
        "metrics = ['acc']\n",
        "\n",
        "# X_train shape: (3375, 60483)\n",
        "# X_test shape:  (1125, 60483)\n",
        "# Y_train shape: (3375,1)\n",
        "# Y_test shape:  (1125,1)\n",
        "\n",
        "# 60483\n",
        "x_train_len = X_train.shape[1]   \n",
        "\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "# X_train shape: (3375, 60483, 1)\n",
        "# X_test shape:  (1125, 60483, 1)\n",
        "\n",
        "\n",
        "filters = 128 \n",
        "filter_len = 20 \n",
        "stride = 1 \n",
        "\n",
        "# inside pool_list loop\n",
        "pool_list = [1,10]\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# model.add  CONV1D\n",
        "model.add(Conv1D(filters = filters, \n",
        "                 kernel_size = filter_len, \n",
        "                 strides = stride, \n",
        "                 padding='valid', \n",
        "                 input_shape=(x_train_len, 1)))\n",
        "\n",
        "# x_train_len = 60,483\n",
        "# Activation\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# MaxPooling\n",
        "model.add(MaxPooling1D(pool_size = 1))\n",
        "\n",
        "filters = 128\n",
        "filter_len = 10 \n",
        "stride = 1 \n",
        "# Conv1D\n",
        "model.add(Conv1D(filters=filters, \n",
        "                 kernel_size=filter_len, \n",
        "                 strides=stride, \n",
        "                 padding='valid'))\n",
        "# Activation\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# MaxPooling\n",
        "model.add(MaxPooling1D(pool_size = 10))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(200))\n",
        "\n",
        "# activation \n",
        "# model.add(Activation('relu')) # SR\n",
        "model.add(Activation(activation))\n",
        "#dropout\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(20))\n",
        "# activation\n",
        "# model.add(Activation('relu')) # SR\n",
        "model.add(Activation(activation))\n",
        "\n",
        "#dropout\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(15))\n",
        "model.add(Activation(out_act))\n",
        "\n",
        "model.compile( loss= loss, \n",
        "              optimizer = optimizer, \n",
        "              metrics = metrics )\n",
        "model.summary()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 60464, 128)        2688      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 60464, 128)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 60464, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 60455, 128)        163968    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 60455, 128)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 6045, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 773760)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               154752200 \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                4020      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 15)                315       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 154,923,191\n",
            "Trainable params: 154,923,191\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "7VCxYIYozqUq",
        "outputId": "cc8b3de3-101b-4167-8a2e-3f3725ea391a"
      },
      "source": [
        "# save\n",
        "save = '.'\n",
        "output_dir = \"/content/drive/My Drive/FNL_TC1/Model\"\n",
        "          \n",
        "output_dir = save \n",
        "if not os.path.exists(output_dir): \n",
        "\tos.makedirs(output_dir)\n",
        "\n",
        "model_name = 'tc1'\n",
        "path = '{}/{}.autosave.model.h5'.format(output_dir, model_name)\n",
        "checkpointer = ModelCheckpoint(filepath=path, \n",
        "                               verbose=1, \n",
        "                               save_weights_only=False, \n",
        "                               save_best_only=True)\n",
        "          \n",
        "csv_logger = CSVLogger('{}/training.log'.format(output_dir))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-aaa4556f1a07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wvSm3M5zqXy"
      },
      "source": [
        "# SR: change epsilon to min_delta\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
        "                              factor=0.1, \n",
        "                              patience=10, \n",
        "                              verbose=1, mode='auto', \n",
        "                              min_delta=0.0001, \n",
        "                              cooldown=0, \n",
        "                              min_lr=0)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnNqZLiNzqaq",
        "outputId": "0a3e9f24-cd03-4cdb-c105-055af097e7dd"
      },
      "source": [
        "# batch_size = 20 \n",
        "history = model.fit(X_train, Y_train, batch_size=batch_size, \n",
        "                    epochs=epochs, verbose=1, validation_data=(X_test, Y_test), \n",
        "                    callbacks = [checkpointer, csv_logger, reduce_lr])\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 2.6672 - acc: 0.0945 \n",
            "Epoch 00001: val_loss improved from inf to 2.52339, saving model to ./tc1.autosave.model.h5\n",
            "169/169 [==============================] - 1952s 12s/step - loss: 2.6672 - acc: 0.0945 - val_loss: 2.5234 - val_acc: 0.1324\n",
            "Epoch 2/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 2.4498 - acc: 0.1890 \n",
            "Epoch 00002: val_loss improved from 2.52339 to 1.69218, saving model to ./tc1.autosave.model.h5\n",
            "169/169 [==============================] - 1958s 12s/step - loss: 2.4498 - acc: 0.1890 - val_loss: 1.6922 - val_acc: 0.4364\n",
            "Epoch 3/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 1.3611 - acc: 0.5399 \n",
            "Epoch 00003: val_loss improved from 1.69218 to 0.90160, saving model to ./tc1.autosave.model.h5\n",
            "169/169 [==============================] - 1937s 11s/step - loss: 1.3611 - acc: 0.5399 - val_loss: 0.9016 - val_acc: 0.6560\n",
            "Epoch 4/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.6145 - acc: 0.7769 \n",
            "Epoch 00004: val_loss improved from 0.90160 to 0.34641, saving model to ./tc1.autosave.model.h5\n",
            "169/169 [==============================] - 1957s 12s/step - loss: 0.6145 - acc: 0.7769 - val_loss: 0.3464 - val_acc: 0.8800\n",
            "Epoch 5/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.3667 - acc: 0.8812 \n",
            "Epoch 00005: val_loss improved from 0.34641 to 0.34408, saving model to ./tc1.autosave.model.h5\n",
            "169/169 [==============================] - 1963s 12s/step - loss: 0.3667 - acc: 0.8812 - val_loss: 0.3441 - val_acc: 0.8818\n",
            "Epoch 6/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.2740 - acc: 0.9096 \n",
            "Epoch 00006: val_loss improved from 0.34408 to 0.24723, saving model to ./tc1.autosave.model.h5\n",
            "169/169 [==============================] - 1963s 12s/step - loss: 0.2740 - acc: 0.9096 - val_loss: 0.2472 - val_acc: 0.9253\n",
            "Epoch 7/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.2033 - acc: 0.9410 \n",
            "Epoch 00007: val_loss improved from 0.24723 to 0.20552, saving model to ./tc1.autosave.model.h5\n",
            "169/169 [==============================] - 1967s 12s/step - loss: 0.2033 - acc: 0.9410 - val_loss: 0.2055 - val_acc: 0.9538\n",
            "Epoch 8/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1590 - acc: 0.9499 \n",
            "Epoch 00008: val_loss improved from 0.20552 to 0.19653, saving model to ./tc1.autosave.model.h5\n",
            "169/169 [==============================] - 1952s 12s/step - loss: 0.1590 - acc: 0.9499 - val_loss: 0.1965 - val_acc: 0.9458\n",
            "Epoch 9/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1345 - acc: 0.9585 \n",
            "Epoch 00009: val_loss improved from 0.19653 to 0.15096, saving model to ./tc1.autosave.model.h5\n",
            "169/169 [==============================] - 1947s 12s/step - loss: 0.1345 - acc: 0.9585 - val_loss: 0.1510 - val_acc: 0.9573\n",
            "Epoch 10/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1177 - acc: 0.9627 \n",
            "Epoch 00010: val_loss did not improve from 0.15096\n",
            "169/169 [==============================] - 1954s 12s/step - loss: 0.1177 - acc: 0.9627 - val_loss: 0.2230 - val_acc: 0.9493\n",
            "Epoch 11/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0974 - acc: 0.9662 \n",
            "Epoch 00011: val_loss did not improve from 0.15096\n",
            "169/169 [==============================] - 1948s 12s/step - loss: 0.0974 - acc: 0.9662 - val_loss: 0.1789 - val_acc: 0.9582\n",
            "Epoch 12/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0729 - acc: 0.9751 \n",
            "Epoch 00012: val_loss did not improve from 0.15096\n",
            "169/169 [==============================] - 1944s 12s/step - loss: 0.0729 - acc: 0.9751 - val_loss: 0.3800 - val_acc: 0.9147\n",
            "Epoch 13/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0646 - acc: 0.9772 \n",
            "Epoch 00013: val_loss did not improve from 0.15096\n",
            "169/169 [==============================] - 1932s 11s/step - loss: 0.0646 - acc: 0.9772 - val_loss: 0.2678 - val_acc: 0.9493\n",
            "Epoch 14/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0550 - acc: 0.9828 \n",
            "Epoch 00014: val_loss did not improve from 0.15096\n",
            "169/169 [==============================] - 1924s 11s/step - loss: 0.0550 - acc: 0.9828 - val_loss: 0.1688 - val_acc: 0.9636\n",
            "Epoch 15/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0501 - acc: 0.9828 \n",
            "Epoch 00015: val_loss did not improve from 0.15096\n",
            "169/169 [==============================] - 1928s 11s/step - loss: 0.0501 - acc: 0.9828 - val_loss: 0.1921 - val_acc: 0.9636\n",
            "Epoch 16/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0335 - acc: 0.9890 \n",
            "Epoch 00016: val_loss did not improve from 0.15096\n",
            "169/169 [==============================] - 1931s 11s/step - loss: 0.0335 - acc: 0.9890 - val_loss: 0.2023 - val_acc: 0.9582\n",
            "Epoch 17/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0671 - acc: 0.9801 \n",
            "Epoch 00017: val_loss did not improve from 0.15096\n",
            "169/169 [==============================] - 1951s 12s/step - loss: 0.0671 - acc: 0.9801 - val_loss: 0.1851 - val_acc: 0.9582\n",
            "Epoch 18/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0390 - acc: 0.9890 \n",
            "Epoch 00018: val_loss did not improve from 0.15096\n",
            "169/169 [==============================] - 1943s 11s/step - loss: 0.0390 - acc: 0.9890 - val_loss: 0.4424 - val_acc: 0.9111\n",
            "Epoch 19/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0240 - acc: 0.9923 \n",
            "Epoch 00019: val_loss did not improve from 0.15096\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
            "169/169 [==============================] - 1935s 11s/step - loss: 0.0240 - acc: 0.9923 - val_loss: 0.2200 - val_acc: 0.9662\n",
            "Epoch 20/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0158 - acc: 0.9935 \n",
            "Epoch 00020: val_loss did not improve from 0.15096\n",
            "169/169 [==============================] - 1953s 12s/step - loss: 0.0158 - acc: 0.9935 - val_loss: 0.1898 - val_acc: 0.9698\n",
            "Epoch 21/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0100 - acc: 0.9964 \n",
            "Epoch 00021: val_loss did not improve from 0.15096\n",
            "169/169 [==============================] - 1966s 12s/step - loss: 0.0100 - acc: 0.9964 - val_loss: 0.2011 - val_acc: 0.9689\n",
            "Epoch 22/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0156 - acc: 0.9956 \n",
            "Epoch 00022: val_loss did not improve from 0.15096\n",
            "169/169 [==============================] - 1963s 12s/step - loss: 0.0156 - acc: 0.9956 - val_loss: 0.2095 - val_acc: 0.9707\n",
            "Epoch 23/50\n",
            "117/169 [===================>..........] - ETA: 9:17 - loss: 0.0196 - acc: 0.9957"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "1eXltMtSRXPm",
        "outputId": "c66d4f54-88a9-48df-f7a6-1d311bcb0a16"
      },
      "source": [
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5170d3b4857d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7t8bvBu0GPF"
      },
      "source": [
        "# Finish up save model weights    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BHLMlpezqdr"
      },
      "source": [
        "# serialize weights to HDF5\n",
        "model.save_weights(\"{}/{}.model.h5\".format(output_dir, model_name))\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model_yaml.load_weights('{}/{}.model.h5'.format(output_dir, model_name))\n",
        "print(\"Loaded yaml model from disk\")\n",
        "\n",
        "# evaluate loaded model on test data\n",
        "loaded_model_yaml.compile(loss=loss,optimizer=optimizer, metrics=metrics) \n",
        "score_yaml = loaded_model_yaml.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print('yaml Test score:', score_yaml[0])\n",
        "print('yaml Test accuracy:', score_yaml[1])\n",
        "\n",
        "print(\"yaml %s: %.2f%%\" % (loaded_model_yaml.metrics_names[1], score_yaml[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OrctuIOzqgh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTEE7F8azqjz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwRb6fXgzqnQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbbipPjizqrW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
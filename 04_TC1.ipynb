{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_TC1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNNbPyp2uTyCL4JZ7+BdD8W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengyaoo/FNL_GenesSelection/blob/main/04_TC1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOdSOBujy5SZ"
      },
      "source": [
        "# Load the libraries\n",
        "from __future__ import print_function\n",
        "\n",
        "import os, sys, gzip, glob, json, time, argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from pandas.io.json import json_normalize\n",
        "\n",
        "from pandas.io.json import json_normalize\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Conv1D, MaxPooling1D, Flatten\n",
        "from keras import optimizers\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.models import Sequential, Model, model_from_json, model_from_yaml\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.linear_model import Lasso, LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import sklearn.manifold as sk_manif\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8upmBOzzBiB",
        "outputId": "1ecf7eb4-fdae-4ec2-e1bc-c4ddf8890361"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8im6yPQlzGRz"
      },
      "source": [
        "# Read features and output files \n",
        "TC1data15 = pd.read_csv(\"/content/drive/My Drive/FNL_TC1/TC1-S1-data15-genename.tsv\", sep=\"\\t\", low_memory = False)\n",
        "outcome = pd.read_csv('/content/drive/My Drive/FNL_TC1/TC1-outcome-data15-projectname.tsv', sep='\\t')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhYQ0BrDzH6N"
      },
      "source": [
        "def encode(data): \n",
        "    print('Shape of data (BEFORE encode): %s' % str(data.shape))\n",
        "    encoded = to_categorical(data)\n",
        "    print('Shape of data (AFTER  encode): %s\\n' % str(encoded.shape))\n",
        "    return encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_1Vevs_zO5r",
        "outputId": "b2d1553f-a7cc-431c-a1ad-13cc09662b67"
      },
      "source": [
        "outcome = encode(outcome['Project_id'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data (BEFORE encode): (4500,)\n",
            "Shape of data (AFTER  encode): (4500, 15)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5eU9aOLzufQ"
      },
      "source": [
        "# Train/Test split  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb59wOPrzpnd"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(TC1data15, outcome, \n",
        "                                                    train_size=0.75, \n",
        "                                                    test_size=0.25, \n",
        "                                                    random_state=123, \n",
        "                                                    stratify = outcome)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmcRFK8Ezypu"
      },
      "source": [
        "# CONV1D "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwgw-7fXzqRb",
        "outputId": "13cf5db5-8473-45a9-f586-1d437b117a0a"
      },
      "source": [
        "# parameters  \n",
        "activation='relu'\n",
        "batch_size=20\n",
        "# Number of sites\n",
        "classes=15\n",
        "drop = 0.1\n",
        "feature_subsample = 0\n",
        "loss='categorical_crossentropy'\n",
        "# metrics='accuracy'\n",
        "out_act='softmax'\n",
        "pool=[1, 10]\n",
        "# optimizer='sgd'\n",
        "shuffle = False \n",
        "epochs=50\n",
        "\n",
        "optimizer = optimizers.SGD(lr=0.1)\n",
        "metrics = ['acc']\n",
        "\n",
        "# X_train shape: (3375, 60483)\n",
        "# X_test shape:  (1125, 60483)\n",
        "# Y_train shape: (3375,1)\n",
        "# Y_test shape:  (1125,1)\n",
        "\n",
        "# 60483\n",
        "x_train_len = X_train.shape[1]   \n",
        "\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "# X_train shape: (3375, 60483, 1)\n",
        "# X_test shape:  (1125, 60483, 1)\n",
        "\n",
        "\n",
        "filters = 128 \n",
        "filter_len = 20 \n",
        "stride = 1 \n",
        "\n",
        "# inside pool_list loop\n",
        "pool_list = [1,10]\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# model.add  CONV1D\n",
        "model.add(Conv1D(filters = filters, \n",
        "                 kernel_size = filter_len, \n",
        "                 strides = stride, \n",
        "                 padding='valid', \n",
        "                 input_shape=(x_train_len, 1)))\n",
        "\n",
        "# x_train_len = 60,483\n",
        "# Activation\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# MaxPooling\n",
        "model.add(MaxPooling1D(pool_size = 1))\n",
        "\n",
        "filters = 128\n",
        "filter_len = 10 \n",
        "stride = 1 \n",
        "# Conv1D\n",
        "model.add(Conv1D(filters=filters, \n",
        "                 kernel_size=filter_len, \n",
        "                 strides=stride, \n",
        "                 padding='valid'))\n",
        "# Activation\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# MaxPooling\n",
        "model.add(MaxPooling1D(pool_size = 10))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(200))\n",
        "\n",
        "# activation \n",
        "# model.add(Activation('relu')) # SR\n",
        "model.add(Activation(activation))\n",
        "#dropout\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(20))\n",
        "# activation\n",
        "# model.add(Activation('relu')) # SR\n",
        "model.add(Activation(activation))\n",
        "\n",
        "#dropout\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(15))\n",
        "model.add(Activation(out_act))\n",
        "\n",
        "model.compile( loss= loss, \n",
        "              optimizer = optimizer, \n",
        "              metrics = metrics )\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 60464, 128)        2688      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 60464, 128)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 60464, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 60455, 128)        163968    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 60455, 128)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 6045, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 773760)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               154752200 \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                4020      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 15)                315       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 154,923,191\n",
            "Trainable params: 154,923,191\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VCxYIYozqUq"
      },
      "source": [
        "# save\n",
        "save = '.'\n",
        "output_dir = \"/content/drive/My Drive/FNL_TC1/Model\"\n",
        "          \n",
        "if not os.path.exists(output_dir): \n",
        "\tos.makedirs(output_dir)\n",
        "\n",
        "model_name = 'tc1'\n",
        "path = '{}/{}.autosave.model.h5'.format(output_dir, model_name)\n",
        "checkpointer = ModelCheckpoint(filepath=path, \n",
        "                               verbose=1, \n",
        "                               save_weights_only=False, \n",
        "                               save_best_only=True)\n",
        "          \n",
        "csv_logger = CSVLogger('{}/training.log'.format(output_dir))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wvSm3M5zqXy"
      },
      "source": [
        "# SR: change epsilon to min_delta\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
        "                              factor=0.1, \n",
        "                              patience=10, \n",
        "                              verbose=1, mode='auto', \n",
        "                              min_delta=0.0001, \n",
        "                              cooldown=0, \n",
        "                              min_lr=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnNqZLiNzqaq",
        "outputId": "ce542e89-5280-4e1f-dc6c-4a498f780b80"
      },
      "source": [
        "# batch_size = 20 \n",
        "history = model.fit(X_train, Y_train, batch_size=batch_size, \n",
        "                    epochs=epochs, verbose=1, validation_data=(X_test, Y_test), \n",
        "                    callbacks = [checkpointer, csv_logger, reduce_lr])\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 2.6475 - acc: 0.1425 \n",
            "Epoch 00001: val_loss improved from inf to 1.86886, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "169/169 [==============================] - 2695s 16s/step - loss: 2.6475 - acc: 0.1425 - val_loss: 1.8689 - val_acc: 0.2684\n",
            "Epoch 2/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 1.4016 - acc: 0.5446 \n",
            "Epoch 00002: val_loss improved from 1.86886 to 0.66994, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "169/169 [==============================] - 2713s 16s/step - loss: 1.4016 - acc: 0.5446 - val_loss: 0.6699 - val_acc: 0.7467\n",
            "Epoch 3/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.5025 - acc: 0.8320 \n",
            "Epoch 00003: val_loss improved from 0.66994 to 0.33216, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "169/169 [==============================] - 2554s 15s/step - loss: 0.5025 - acc: 0.8320 - val_loss: 0.3322 - val_acc: 0.8951\n",
            "Epoch 4/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.3598 - acc: 0.8856 \n",
            "Epoch 00004: val_loss improved from 0.33216 to 0.23838, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "169/169 [==============================] - 2480s 15s/step - loss: 0.3598 - acc: 0.8856 - val_loss: 0.2384 - val_acc: 0.9387\n",
            "Epoch 5/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.2215 - acc: 0.9330 \n",
            "Epoch 00005: val_loss did not improve from 0.23838\n",
            "169/169 [==============================] - 2474s 15s/step - loss: 0.2215 - acc: 0.9330 - val_loss: 0.2446 - val_acc: 0.9324\n",
            "Epoch 6/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1787 - acc: 0.9479 \n",
            "Epoch 00006: val_loss did not improve from 0.23838\n",
            "169/169 [==============================] - 2498s 15s/step - loss: 0.1787 - acc: 0.9479 - val_loss: 0.2393 - val_acc: 0.9227\n",
            "Epoch 7/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1263 - acc: 0.9633 \n",
            "Epoch 00007: val_loss improved from 0.23838 to 0.15950, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "169/169 [==============================] - 2500s 15s/step - loss: 0.1263 - acc: 0.9633 - val_loss: 0.1595 - val_acc: 0.9573\n",
            "Epoch 8/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1091 - acc: 0.9671 \n",
            "Epoch 00008: val_loss did not improve from 0.15950\n",
            "169/169 [==============================] - 2491s 15s/step - loss: 0.1091 - acc: 0.9671 - val_loss: 0.1646 - val_acc: 0.9600\n",
            "Epoch 9/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0900 - acc: 0.9754 \n",
            "Epoch 00009: val_loss did not improve from 0.15950\n",
            "169/169 [==============================] - 2487s 15s/step - loss: 0.0900 - acc: 0.9754 - val_loss: 0.1927 - val_acc: 0.9502\n",
            "Epoch 10/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0933 - acc: 0.9713 \n",
            "Epoch 00010: val_loss did not improve from 0.15950\n",
            "169/169 [==============================] - 2490s 15s/step - loss: 0.0933 - acc: 0.9713 - val_loss: 0.2357 - val_acc: 0.9271\n",
            "Epoch 11/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0859 - acc: 0.9710 \n",
            "Epoch 00011: val_loss did not improve from 0.15950\n",
            "169/169 [==============================] - 2484s 15s/step - loss: 0.0859 - acc: 0.9710 - val_loss: 0.2217 - val_acc: 0.9493\n",
            "Epoch 12/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0474 - acc: 0.9837 \n",
            "Epoch 00012: val_loss did not improve from 0.15950\n",
            "169/169 [==============================] - 2484s 15s/step - loss: 0.0474 - acc: 0.9837 - val_loss: 0.1696 - val_acc: 0.9662\n",
            "Epoch 13/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0492 - acc: 0.9825 \n",
            "Epoch 00013: val_loss improved from 0.15950 to 0.15184, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1.autosave.model.h5\n",
            "169/169 [==============================] - 2490s 15s/step - loss: 0.0492 - acc: 0.9825 - val_loss: 0.1518 - val_acc: 0.9662\n",
            "Epoch 14/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0497 - acc: 0.9846 \n",
            "Epoch 00014: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2494s 15s/step - loss: 0.0497 - acc: 0.9846 - val_loss: 0.1772 - val_acc: 0.9644\n",
            "Epoch 15/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0345 - acc: 0.9884 \n",
            "Epoch 00015: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2483s 15s/step - loss: 0.0345 - acc: 0.9884 - val_loss: 0.2040 - val_acc: 0.9591\n",
            "Epoch 16/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0294 - acc: 0.9926 \n",
            "Epoch 00016: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2487s 15s/step - loss: 0.0294 - acc: 0.9926 - val_loss: 0.1556 - val_acc: 0.9591\n",
            "Epoch 17/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0235 - acc: 0.9938 \n",
            "Epoch 00017: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2491s 15s/step - loss: 0.0235 - acc: 0.9938 - val_loss: 0.2504 - val_acc: 0.9591\n",
            "Epoch 18/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0324 - acc: 0.9908 \n",
            "Epoch 00018: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2493s 15s/step - loss: 0.0324 - acc: 0.9908 - val_loss: 0.4318 - val_acc: 0.9191\n",
            "Epoch 19/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0257 - acc: 0.9914 \n",
            "Epoch 00019: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2494s 15s/step - loss: 0.0257 - acc: 0.9914 - val_loss: 0.1784 - val_acc: 0.9627\n",
            "Epoch 20/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0230 - acc: 0.9935 \n",
            "Epoch 00020: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2489s 15s/step - loss: 0.0230 - acc: 0.9935 - val_loss: 0.2068 - val_acc: 0.9582\n",
            "Epoch 21/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0202 - acc: 0.9935 \n",
            "Epoch 00021: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2502s 15s/step - loss: 0.0202 - acc: 0.9935 - val_loss: 0.2250 - val_acc: 0.9680\n",
            "Epoch 22/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0328 - acc: 0.9905 \n",
            "Epoch 00022: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2500s 15s/step - loss: 0.0328 - acc: 0.9905 - val_loss: 0.1587 - val_acc: 0.9680\n",
            "Epoch 23/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0273 - acc: 0.9914 \n",
            "Epoch 00023: val_loss did not improve from 0.15184\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
            "169/169 [==============================] - 2498s 15s/step - loss: 0.0273 - acc: 0.9914 - val_loss: 0.2154 - val_acc: 0.9564\n",
            "Epoch 24/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0142 - acc: 0.9961 \n",
            "Epoch 00024: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2495s 15s/step - loss: 0.0142 - acc: 0.9961 - val_loss: 0.1691 - val_acc: 0.9689\n",
            "Epoch 25/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0111 - acc: 0.9976 \n",
            "Epoch 00025: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2498s 15s/step - loss: 0.0111 - acc: 0.9976 - val_loss: 0.1769 - val_acc: 0.9707\n",
            "Epoch 26/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0049 - acc: 0.9991 \n",
            "Epoch 00026: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2498s 15s/step - loss: 0.0049 - acc: 0.9991 - val_loss: 0.1803 - val_acc: 0.9680\n",
            "Epoch 27/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0067 - acc: 0.9988 \n",
            "Epoch 00027: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2504s 15s/step - loss: 0.0067 - acc: 0.9988 - val_loss: 0.1843 - val_acc: 0.9689\n",
            "Epoch 28/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0073 - acc: 0.9985 \n",
            "Epoch 00028: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2503s 15s/step - loss: 0.0073 - acc: 0.9985 - val_loss: 0.1881 - val_acc: 0.9698\n",
            "Epoch 29/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0039 - acc: 0.9988 \n",
            "Epoch 00029: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2499s 15s/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.1913 - val_acc: 0.9689\n",
            "Epoch 30/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0032 - acc: 0.9994 \n",
            "Epoch 00030: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2488s 15s/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.1969 - val_acc: 0.9689\n",
            "Epoch 31/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0053 - acc: 0.9988 \n",
            "Epoch 00031: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2487s 15s/step - loss: 0.0053 - acc: 0.9988 - val_loss: 0.2037 - val_acc: 0.9698\n",
            "Epoch 32/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0049 - acc: 0.9985 \n",
            "Epoch 00032: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2489s 15s/step - loss: 0.0049 - acc: 0.9985 - val_loss: 0.2066 - val_acc: 0.9689\n",
            "Epoch 33/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0043 - acc: 0.9988 \n",
            "Epoch 00033: val_loss did not improve from 0.15184\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "169/169 [==============================] - 2492s 15s/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.2067 - val_acc: 0.9698\n",
            "Epoch 34/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0038 - acc: 0.9988 \n",
            "Epoch 00034: val_loss did not improve from 0.15184\n",
            "169/169 [==============================] - 2489s 15s/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.2068 - val_acc: 0.9698\n",
            "Epoch 35/50\n",
            " 58/169 [=========>....................] - ETA: 24:45 - loss: 0.0073 - acc: 0.9983"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eXltMtSRXPm"
      },
      "source": [
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLUPT84mCQUQ"
      },
      "source": [
        "Y_pred=np.argmax(model.predict(x_test),axis=-1)\n",
        "acc_val=np.mean(Y_pred==Y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7t8bvBu0GPF"
      },
      "source": [
        "# Finish up save model weights    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BHLMlpezqdr"
      },
      "source": [
        "# serialize weights to HDF5\n",
        "model.save_weights(\"{}/{}.model.h5\".format(output_dir, model_name))\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model_yaml.load_weights('{}/{}.model.h5'.format(output_dir, model_name))\n",
        "print(\"Loaded yaml model from disk\")\n",
        "\n",
        "# evaluate loaded model on test data\n",
        "loaded_model_yaml.compile(loss=loss,optimizer=optimizer, metrics=metrics) \n",
        "score_yaml = loaded_model_yaml.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print('yaml Test score:', score_yaml[0])\n",
        "print('yaml Test accuracy:', score_yaml[1])\n",
        "\n",
        "print(\"yaml %s: %.2f%%\" % (loaded_model_yaml.metrics_names[1], score_yaml[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OrctuIOzqgh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTEE7F8azqjz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwRb6fXgzqnQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbbipPjizqrW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
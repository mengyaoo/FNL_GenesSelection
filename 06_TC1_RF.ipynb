{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_TC1_RF.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPOaz8EmaJT+2eVpvsIc/ME",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengyaoo/FNL_GenesSelection/blob/main/06_TC1_RF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-iE_t7nyh1Q"
      },
      "source": [
        "# Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-23T11:46:55.156880Z",
          "start_time": "2020-11-23T11:46:54.748422Z"
        },
        "id": "V5Zg0wifMGGJ"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os, sys, gzip, glob, json, time, argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from pandas.io.json import json_normalize\n",
        "\n",
        "from pandas.io.json import json_normalize\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Conv1D, MaxPooling1D, Flatten\n",
        "from keras import optimizers\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.models import Sequential, Model, model_from_json, model_from_yaml\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.linear_model import Lasso, LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import sklearn.manifold as sk_manif\n",
        "from keras.utils import to_categorical\n",
        "from google.colab import drive\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwUo3_68MGGJ"
      },
      "source": [
        "# Data Preparation   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNRDQKlmiyGw",
        "outputId": "c7ecd766-61ee-4dc2-bdc9-c24e1da815e7"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-23T06:24:03.697460Z",
          "start_time": "2020-11-23T06:04:41.100012Z"
        },
        "id": "f8SDzPeuMGGK"
      },
      "source": [
        "# Read features and output files \n",
        "TC1data15 = pd.read_csv(\"/content/drive/My Drive/FNL_TC1/TC1-S1-data15-genename.tsv\", sep=\"\\t\", low_memory = False)\n",
        "#TC1data15 = sfeatures1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-23T06:24:03.787859Z",
          "start_time": "2020-11-23T06:24:03.727784Z"
        },
        "id": "955mydtlMGGK"
      },
      "source": [
        "outcome = pd.read_csv('/content/drive/My Drive/FNL_TC1/TC1-outcome-data15-projectname.tsv', sep='\\t')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEMDGF4K7D4B"
      },
      "source": [
        "def encode(data): \n",
        "    print('Shape of data (BEFORE encode): %s' % str(data.shape))\n",
        "    encoded = to_categorical(data)\n",
        "    print('Shape of data (AFTER  encode): %s\\n' % str(encoded.shape))\n",
        "    return encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_Relc1R1EnL"
      },
      "source": [
        "# outcome = encode(outcome['Project_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d3-7dxFyH8c"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uzfRSFtsJst",
        "outputId": "3f425fe5-7797-44b2-a1ba-83ac3e28ae72"
      },
      "source": [
        "np.random.seed(123)\n",
        "# define the model\n",
        "model = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
        "# fit the model\n",
        "model.fit(TC1data15, outcome['Project_id'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEH8VH2myVWl"
      },
      "source": [
        "# get importance\n",
        "importance = model.feature_importances_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlEuwnboz8EF"
      },
      "source": [
        "feature = pd.DataFrame()\n",
        "feature['name'] = TC1data15.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "JW-IKLjYz5XQ",
        "outputId": "eb1f675c-f766-4707-814c-1065c4bdc43f"
      },
      "source": [
        "feature['importance'] = importance\n",
        "feature"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TSPAN6</td>\n",
              "      <td>0.000004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TNMD</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DPM1</td>\n",
              "      <td>0.000039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SCYL3</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C1orf112</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60478</th>\n",
              "      <td>Metazoa_SRP.305</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60479</th>\n",
              "      <td>AJ271736.1.1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60480</th>\n",
              "      <td>MIR6089.1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60481</th>\n",
              "      <td>RP13-465B17.5.1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60482</th>\n",
              "      <td>RP13-465B17.4.1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60483 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  name  importance\n",
              "0               TSPAN6    0.000004\n",
              "1                 TNMD    0.000000\n",
              "2                 DPM1    0.000039\n",
              "3                SCYL3    0.000000\n",
              "4             C1orf112    0.000002\n",
              "...                ...         ...\n",
              "60478  Metazoa_SRP.305    0.000000\n",
              "60479     AJ271736.1.1    0.000000\n",
              "60480        MIR6089.1    0.000000\n",
              "60481  RP13-465B17.5.1    0.000000\n",
              "60482  RP13-465B17.4.1    0.000000\n",
              "\n",
              "[60483 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NJhcMKL1eeW"
      },
      "source": [
        "feature_sort = feature.sort_values(by='importance',ascending=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t4JMCrz2Cak"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "68AuDd4U3Zs4",
        "outputId": "595a7ddc-aee3-4078-ee67-bb2ecfab3919"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10258</th>\n",
              "      <td>SLC45A3</td>\n",
              "      <td>0.003607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36533</th>\n",
              "      <td>CTD-2182N23.1</td>\n",
              "      <td>0.003576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29180</th>\n",
              "      <td>AC012123.1</td>\n",
              "      <td>0.003306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6445</th>\n",
              "      <td>NAPSA</td>\n",
              "      <td>0.002897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12521</th>\n",
              "      <td>C8orf46</td>\n",
              "      <td>0.002802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37013</th>\n",
              "      <td>HNF1A-AS1</td>\n",
              "      <td>0.001027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25514</th>\n",
              "      <td>TMSB4XP4</td>\n",
              "      <td>0.001024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60030</th>\n",
              "      <td>CH507-513H4.3</td>\n",
              "      <td>0.001022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16469</th>\n",
              "      <td>BCAM</td>\n",
              "      <td>0.001012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37745</th>\n",
              "      <td>RP11-115N4.1</td>\n",
              "      <td>0.001003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>129 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                name  importance\n",
              "10258        SLC45A3    0.003607\n",
              "36533  CTD-2182N23.1    0.003576\n",
              "29180     AC012123.1    0.003306\n",
              "6445           NAPSA    0.002897\n",
              "12521        C8orf46    0.002802\n",
              "...              ...         ...\n",
              "37013      HNF1A-AS1    0.001027\n",
              "25514       TMSB4XP4    0.001024\n",
              "60030  CH507-513H4.3    0.001022\n",
              "16469           BCAM    0.001012\n",
              "37745   RP11-115N4.1    0.001003\n",
              "\n",
              "[129 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvc0DUNEFbxk"
      },
      "source": [
        "# ConvNN(RF) - top 129"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h4agjwbFbxl"
      },
      "source": [
        "## Train/Test split    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0niWyzNw6Ax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ad5e4c-26be-4bd5-a819-3f12273a2a7a"
      },
      "source": [
        "# from keras.utils import to_categorical\n",
        "outcome = encode(outcome['Project_id'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data (BEFORE encode): (4500,)\n",
            "Shape of data (AFTER  encode): (4500, 15)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT6lILqx6g5W"
      },
      "source": [
        "feature_sort_selected = feature_sort[feature_sort['importance'] > 0.001]\n",
        "feature_sort_selected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS9D16qIFbxn"
      },
      "source": [
        "TC1data15_selected = TC1data15.loc[:,feature_sort_selected['name']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PWsfc3nFbxo"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(TC1data15_selected, \n",
        "                                                    outcome, \n",
        "                                                    train_size=0.75, \n",
        "                                                    test_size=0.25, \n",
        "                                                    random_state=123, \n",
        "                                                    stratify = outcome)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "ELJV_czkHjbj",
        "outputId": "78ba8ebc-0caa-473e-e5ea-f1c6bd5e1243"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SLC45A3</th>\n",
              "      <th>CTD-2182N23.1</th>\n",
              "      <th>AC012123.1</th>\n",
              "      <th>NAPSA</th>\n",
              "      <th>C8orf46</th>\n",
              "      <th>AL161626.1</th>\n",
              "      <th>RP11-264B14.2</th>\n",
              "      <th>FP236383.9</th>\n",
              "      <th>SOX17</th>\n",
              "      <th>ACSM2B</th>\n",
              "      <th>PAX8</th>\n",
              "      <th>U47924.27</th>\n",
              "      <th>PSD2</th>\n",
              "      <th>FP671120.6</th>\n",
              "      <th>TRPS1</th>\n",
              "      <th>NKX2-1-AS1</th>\n",
              "      <th>CHRNA2</th>\n",
              "      <th>TSHR</th>\n",
              "      <th>GAL3ST1</th>\n",
              "      <th>SLC28A1</th>\n",
              "      <th>APOC3</th>\n",
              "      <th>Y_RNA.262</th>\n",
              "      <th>C8B</th>\n",
              "      <th>SNRPGP10</th>\n",
              "      <th>RP11-25I15.1</th>\n",
              "      <th>SLC17A3</th>\n",
              "      <th>SLC2A2</th>\n",
              "      <th>SLC39A5</th>\n",
              "      <th>TM4SF5</th>\n",
              "      <th>PCA3</th>\n",
              "      <th>RMST</th>\n",
              "      <th>ATP8A2P1</th>\n",
              "      <th>MGAT4C</th>\n",
              "      <th>HEPH</th>\n",
              "      <th>ORM2</th>\n",
              "      <th>PLG</th>\n",
              "      <th>CRYGN</th>\n",
              "      <th>RP11-53M11.5</th>\n",
              "      <th>TRABD2A</th>\n",
              "      <th>IYD</th>\n",
              "      <th>...</th>\n",
              "      <th>TTC21B-AS1</th>\n",
              "      <th>BHMT2</th>\n",
              "      <th>NAT8</th>\n",
              "      <th>FRMD3</th>\n",
              "      <th>SLC22A31</th>\n",
              "      <th>HAO1</th>\n",
              "      <th>SFTPA1</th>\n",
              "      <th>WT1</th>\n",
              "      <th>IRX5</th>\n",
              "      <th>MUC15</th>\n",
              "      <th>RP11-114N19.3</th>\n",
              "      <th>GPA33</th>\n",
              "      <th>MYO1A</th>\n",
              "      <th>DGKA</th>\n",
              "      <th>RP11-13J10.1</th>\n",
              "      <th>HPX</th>\n",
              "      <th>RP11-710C12.1</th>\n",
              "      <th>FER1L4</th>\n",
              "      <th>FUT4</th>\n",
              "      <th>SEMA5B</th>\n",
              "      <th>RAPGEF3</th>\n",
              "      <th>HNF4A</th>\n",
              "      <th>ANKS4B</th>\n",
              "      <th>SLC5A10</th>\n",
              "      <th>CUX2</th>\n",
              "      <th>FLJ20021</th>\n",
              "      <th>HOXA11</th>\n",
              "      <th>SLC26A4-AS1</th>\n",
              "      <th>RP11-108P20.4</th>\n",
              "      <th>ARHGEF38</th>\n",
              "      <th>CREB3L4</th>\n",
              "      <th>SLC22A6</th>\n",
              "      <th>GATA3</th>\n",
              "      <th>SLC17A1</th>\n",
              "      <th>KCNN2</th>\n",
              "      <th>HNF1A-AS1</th>\n",
              "      <th>TMSB4XP4</th>\n",
              "      <th>CH507-513H4.3</th>\n",
              "      <th>BCAM</th>\n",
              "      <th>RP11-115N4.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>849</th>\n",
              "      <td>0.717435</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.672119</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.424186</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.170832</td>\n",
              "      <td>1.493830</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.094042</td>\n",
              "      <td>1.026745</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.924745</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079015</td>\n",
              "      <td>0.615134</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.229850</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.752871</td>\n",
              "      <td>1.765075</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.184754</td>\n",
              "      <td>0.529635</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014796</td>\n",
              "      <td>2.716618</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.167719</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.780820</td>\n",
              "      <td>0.812019</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.943531</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.323472</td>\n",
              "      <td>0.586924</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.876287</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.210308</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.571586</td>\n",
              "      <td>1.407091</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.417708</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.268890</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.752059</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.732174</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1050</th>\n",
              "      <td>0.901859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.732119</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.257574</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.420687</td>\n",
              "      <td>0.861244</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.566138</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.137554</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.473346</td>\n",
              "      <td>0.452157</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.780366</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.993561</td>\n",
              "      <td>2.947773</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.980522</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.472642</td>\n",
              "      <td>0.249830</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.884083</td>\n",
              "      <td>1.840286</td>\n",
              "      <td>0.217037</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.396074</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.038801</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.245857</td>\n",
              "      <td>1.068979</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.129735</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.632624</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.867953</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.630890</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.914717</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>1.169069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.690877</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365155</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041478</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.848804</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.157242</td>\n",
              "      <td>0.557529</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.914976</td>\n",
              "      <td>0.622720</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.659029</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.576785</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.997362</td>\n",
              "      <td>0.640531</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.822078</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.228457</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.650705</td>\n",
              "      <td>0.213690</td>\n",
              "      <td>0.063357</td>\n",
              "      <td>0.234935</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.399494</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.316965</td>\n",
              "      <td>1.276605</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.159345</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.506625</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4447</th>\n",
              "      <td>1.001501</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.393282</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.284720</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.510884</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039429</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.537337</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.589360</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.386256</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.020446</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.076092</td>\n",
              "      <td>0.235987</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.217671</td>\n",
              "      <td>0.929124</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.853477</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.711598</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.756108</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.920132</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0.429030</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.698742</td>\n",
              "      <td>0.094371</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.670652</td>\n",
              "      <td>0.708519</td>\n",
              "      <td>2.400472</td>\n",
              "      <td>2.316836</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.962383</td>\n",
              "      <td>1.967081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.948668</td>\n",
              "      <td>1.550796</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365934</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.092616</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.604855</td>\n",
              "      <td>1.268017</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.480250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.133977</td>\n",
              "      <td>0.475185</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.889395</td>\n",
              "      <td>0.294703</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.166122</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.716844</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.289258</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029921</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.450754</td>\n",
              "      <td>1.161756</td>\n",
              "      <td>1.021279</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.168555</td>\n",
              "      <td>0.233019</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.809214</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.866541</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.668162</td>\n",
              "      <td>0.546792</td>\n",
              "      <td>2.681609</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3495</th>\n",
              "      <td>1.558939</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.636459</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.052139</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.436981</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.659693</td>\n",
              "      <td>0.337474</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.196452</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.168646</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.827239</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.084505</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.301188</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.544882</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.712296</td>\n",
              "      <td>0.788982</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.597645</td>\n",
              "      <td>0.483691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.270596</td>\n",
              "      <td>1.474903</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.187701</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.182879</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.969075</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.334545</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1737</th>\n",
              "      <td>0.508131</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.234141</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.229859</td>\n",
              "      <td>1.537464</td>\n",
              "      <td>1.877514</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.735723</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.093082</td>\n",
              "      <td>1.849123</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.315884</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.722</td>\n",
              "      <td>1.62879</td>\n",
              "      <td>1.310074</td>\n",
              "      <td>1.645405</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.199691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.155142</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.535388</td>\n",
              "      <td>2.118009</td>\n",
              "      <td>2.842803</td>\n",
              "      <td>1.605885</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.969272</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.678423</td>\n",
              "      <td>1.158139</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078102</td>\n",
              "      <td>0.682879</td>\n",
              "      <td>1.849534</td>\n",
              "      <td>0.734579</td>\n",
              "      <td>1.241108</td>\n",
              "      <td>1.475341</td>\n",
              "      <td>1.222774</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.054436</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.134095</td>\n",
              "      <td>1.400276</td>\n",
              "      <td>0.699850</td>\n",
              "      <td>1.540965</td>\n",
              "      <td>0.092485</td>\n",
              "      <td>0.327483</td>\n",
              "      <td>0.453330</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.145418</td>\n",
              "      <td>0.484552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2943</th>\n",
              "      <td>0.794707</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.148528</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.383600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.406436</td>\n",
              "      <td>1.454994</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.320577</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.268603</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.417348</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.399102</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.313431</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.115971</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250513</td>\n",
              "      <td>1.295102</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.630025</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.884104</td>\n",
              "      <td>0.981024</td>\n",
              "      <td>0.269996</td>\n",
              "      <td>1.407186</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.181746</td>\n",
              "      <td>1.477543</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.408404</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.887244</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.205806</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.493061</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1573</th>\n",
              "      <td>0.891176</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.707764</td>\n",
              "      <td>0.152280</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.282959</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.620480</td>\n",
              "      <td>1.605914</td>\n",
              "      <td>0.051819</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.200227</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.238608</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.973891</td>\n",
              "      <td>0.225703</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.158866</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.991760</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.137346</td>\n",
              "      <td>0.301543</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.788676</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.757699</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.822889</td>\n",
              "      <td>0.649706</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.567187</td>\n",
              "      <td>0.338646</td>\n",
              "      <td>0.242642</td>\n",
              "      <td>0.269726</td>\n",
              "      <td>1.565884</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.730327</td>\n",
              "      <td>1.109687</td>\n",
              "      <td>1.022463</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.629490</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.181766</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.535085</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.882158</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3192</th>\n",
              "      <td>0.440866</td>\n",
              "      <td>3.215473</td>\n",
              "      <td>0.649271</td>\n",
              "      <td>0.455298</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.852791</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.665235</td>\n",
              "      <td>1.730446</td>\n",
              "      <td>0.292689</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.725373</td>\n",
              "      <td>1.328306</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.821487</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.222551</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055060</td>\n",
              "      <td>0.227691</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.75023</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.258463</td>\n",
              "      <td>0.762914</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.617423</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.566856</td>\n",
              "      <td>1.581944</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.915972</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.304026</td>\n",
              "      <td>1.993056</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.170563</td>\n",
              "      <td>0.907122</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.853357</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.850287</td>\n",
              "      <td>0.505412</td>\n",
              "      <td>0.450916</td>\n",
              "      <td>1.029259</td>\n",
              "      <td>1.781765</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.205681</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.20682</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.775604</td>\n",
              "      <td>1.270582</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.811412</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.343253</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.561591</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3375 rows × 129 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       SLC45A3  CTD-2182N23.1  ...      BCAM  RP11-115N4.1\n",
              "849   0.717435       0.000000  ...  2.732174      0.000000\n",
              "1050  0.901859       0.000000  ...  1.914717      0.000000\n",
              "835   1.169069       0.000000  ...  2.506625      0.000000\n",
              "4447  1.001501       0.000000  ...  1.920132      0.000000\n",
              "53    0.429030       0.000000  ...  2.681609      0.000000\n",
              "...        ...            ...  ...       ...           ...\n",
              "3495  1.558939       0.000000  ...  2.334545      0.000000\n",
              "1737  0.508131       0.000000  ...  2.145418      0.484552\n",
              "2943  0.794707       0.000000  ...  2.493061      0.000000\n",
              "1573  0.891176       0.000000  ...  2.882158      0.000000\n",
              "3192  0.440866       3.215473  ...  2.561591      0.000000\n",
              "\n",
              "[3375 rows x 129 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4NiRi1SFbxo"
      },
      "source": [
        "## CONV1D "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjD0T5I2Fbxo"
      },
      "source": [
        "# parameters  \n",
        "activation='relu'\n",
        "batch_size=20\n",
        "# Number of sites\n",
        "classes=15\n",
        "drop = 0.1\n",
        "feature_subsample = 0\n",
        "loss='categorical_crossentropy'\n",
        "# metrics='accuracy'\n",
        "out_act='softmax'\n",
        "pool=[1, 10]\n",
        "# optimizer='sgd'\n",
        "shuffle = False \n",
        "epochs=100\n",
        "\n",
        "optimizer = optimizers.SGD(lr=0.1)\n",
        "metrics = ['acc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztdyDFcoTXTH"
      },
      "source": [
        "x_train_len = X_train.shape[1]   \n",
        "\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "filters = 128 \n",
        "filter_len = 20 \n",
        "stride = 1 \n",
        "\n",
        "# inside pool_list loop\n",
        "pool_list = [1,10]\n",
        "\n",
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muxPrNNYFbxp",
        "outputId": "1e07be9a-3848-425a-c5c1-c2164ba15cff"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# model.add  CONV1D\n",
        "model.add(Conv1D(filters = filters, \n",
        "                 kernel_size = filter_len, \n",
        "                 strides = stride, \n",
        "                 padding='valid', \n",
        "                 input_shape=(x_train_len, 1)))\n",
        "\n",
        "# Activation\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# MaxPooling\n",
        "model.add(MaxPooling1D(pool_size = 1))\n",
        "\n",
        "filters = 128\n",
        "filter_len = 10 \n",
        "stride = 1 \n",
        "# Conv1D\n",
        "model.add(Conv1D(filters=filters, \n",
        "                 kernel_size=filter_len, \n",
        "                 strides=stride, \n",
        "                 padding='valid'))\n",
        "# Activation\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# MaxPooling\n",
        "model.add(MaxPooling1D(pool_size = 10))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(200))\n",
        "\n",
        "# activation \n",
        "# model.add(Activation('relu')) # SR\n",
        "model.add(Activation(activation))\n",
        "#dropout\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(20))\n",
        "# activation\n",
        "# model.add(Activation('relu')) # SR\n",
        "model.add(Activation(activation))\n",
        "\n",
        "#dropout\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(15))\n",
        "model.add(Activation(out_act))\n",
        "\n",
        "model.compile( loss= loss, \n",
        "              optimizer = optimizer, \n",
        "              metrics = metrics )\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 110, 128)          2688      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 110, 128)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 110, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 101, 128)          163968    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 101, 128)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 10, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               256200    \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                4020      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 15)                315       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 427,191\n",
            "Trainable params: 427,191\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwoVbhQMFbxp"
      },
      "source": [
        "# save\n",
        "# save = '/content/drive/My Drive/FNL_TC1/'\n",
        "output_dir = \"/content/drive/My Drive/FNL_TC1/Model\"\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "model_name = 'tc1_rf'\n",
        "path = '{}/{}.autosave.model.h5'.format(output_dir, model_name)\n",
        "checkpointer = ModelCheckpoint(filepath=path,\n",
        "                               verbose=1,\n",
        "                               save_weights_only=True,\n",
        "                               save_best_only=True)\n",
        "\n",
        "csv_logger = CSVLogger('{}/training.log'.format(output_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBFfONmUFbxq"
      },
      "source": [
        "# SR: change epsilon to min_delta\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
        "                              factor=0.1, \n",
        "                              patience=10, \n",
        "                              verbose=1, mode='auto', \n",
        "                              min_delta=0.0001, \n",
        "                              cooldown=0, \n",
        "                              min_lr=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVfcLEdgFbxq",
        "outputId": "82ea6077-82f5-41e5-aa79-cacc9fadf062"
      },
      "source": [
        "# batch_size = 20 \n",
        "history = model.fit(X_train, Y_train, batch_size=batch_size, \n",
        "                    epochs=epochs, verbose=1, validation_data=(X_test, Y_test), \n",
        "                    callbacks = [checkpointer, csv_logger, reduce_lr])\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 1.0708 - acc: 0.6726\n",
            "Epoch 00001: val_loss improved from inf to 0.46506, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 1.0675 - acc: 0.6738 - val_loss: 0.4651 - val_acc: 0.8516\n",
            "Epoch 2/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.3445 - acc: 0.8883\n",
            "Epoch 00002: val_loss improved from 0.46506 to 0.27360, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 13s 75ms/step - loss: 0.3445 - acc: 0.8883 - val_loss: 0.2736 - val_acc: 0.9120\n",
            "Epoch 3/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9250\n",
            "Epoch 00003: val_loss improved from 0.27360 to 0.23414, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.2403 - acc: 0.9253 - val_loss: 0.2341 - val_acc: 0.9387\n",
            "Epoch 4/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.2082 - acc: 0.9378\n",
            "Epoch 00004: val_loss improved from 0.23414 to 0.21387, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 12s 74ms/step - loss: 0.2077 - acc: 0.9378 - val_loss: 0.2139 - val_acc: 0.9449\n",
            "Epoch 5/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1833 - acc: 0.9446\n",
            "Epoch 00005: val_loss improved from 0.21387 to 0.21297, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.1833 - acc: 0.9446 - val_loss: 0.2130 - val_acc: 0.9422\n",
            "Epoch 6/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9449\n",
            "Epoch 00006: val_loss did not improve from 0.21297\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.1781 - acc: 0.9449 - val_loss: 0.2411 - val_acc: 0.9351\n",
            "Epoch 7/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9568\n",
            "Epoch 00007: val_loss did not improve from 0.21297\n",
            "169/169 [==============================] - 12s 70ms/step - loss: 0.1411 - acc: 0.9570 - val_loss: 0.2162 - val_acc: 0.9440\n",
            "Epoch 8/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1403 - acc: 0.9570\n",
            "Epoch 00008: val_loss did not improve from 0.21297\n",
            "169/169 [==============================] - 12s 70ms/step - loss: 0.1403 - acc: 0.9570 - val_loss: 0.2439 - val_acc: 0.9369\n",
            "Epoch 9/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1266 - acc: 0.9603\n",
            "Epoch 00009: val_loss did not improve from 0.21297\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.1266 - acc: 0.9603 - val_loss: 0.2147 - val_acc: 0.9529\n",
            "Epoch 10/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1176 - acc: 0.9624\n",
            "Epoch 00010: val_loss did not improve from 0.21297\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.1176 - acc: 0.9624 - val_loss: 0.2281 - val_acc: 0.9404\n",
            "Epoch 11/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9667\n",
            "Epoch 00011: val_loss improved from 0.21297 to 0.20930, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0998 - acc: 0.9668 - val_loss: 0.2093 - val_acc: 0.9458\n",
            "Epoch 12/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9682\n",
            "Epoch 00012: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0946 - acc: 0.9680 - val_loss: 0.2295 - val_acc: 0.9449\n",
            "Epoch 13/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0893 - acc: 0.9721\n",
            "Epoch 00013: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0893 - acc: 0.9721 - val_loss: 0.2236 - val_acc: 0.9484\n",
            "Epoch 14/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0885 - acc: 0.9730\n",
            "Epoch 00014: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0885 - acc: 0.9730 - val_loss: 0.2328 - val_acc: 0.9493\n",
            "Epoch 15/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0813 - acc: 0.9745\n",
            "Epoch 00015: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0813 - acc: 0.9745 - val_loss: 0.2284 - val_acc: 0.9431\n",
            "Epoch 16/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9720\n",
            "Epoch 00016: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0826 - acc: 0.9719 - val_loss: 0.2630 - val_acc: 0.9440\n",
            "Epoch 17/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0696 - acc: 0.9775\n",
            "Epoch 00017: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0696 - acc: 0.9775 - val_loss: 0.2468 - val_acc: 0.9476\n",
            "Epoch 18/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9747\n",
            "Epoch 00018: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 68ms/step - loss: 0.0716 - acc: 0.9748 - val_loss: 0.2475 - val_acc: 0.9484\n",
            "Epoch 19/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0560 - acc: 0.9796\n",
            "Epoch 00019: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 11s 68ms/step - loss: 0.0560 - acc: 0.9796 - val_loss: 0.2543 - val_acc: 0.9440\n",
            "Epoch 20/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0536 - acc: 0.9828\n",
            "Epoch 00020: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 11s 68ms/step - loss: 0.0536 - acc: 0.9828 - val_loss: 0.2876 - val_acc: 0.9493\n",
            "Epoch 21/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9866\n",
            "Epoch 00021: val_loss did not improve from 0.20930\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
            "169/169 [==============================] - 12s 69ms/step - loss: 0.0468 - acc: 0.9864 - val_loss: 0.2727 - val_acc: 0.9449\n",
            "Epoch 22/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9896\n",
            "Epoch 00022: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 69ms/step - loss: 0.0348 - acc: 0.9896 - val_loss: 0.2484 - val_acc: 0.9538\n",
            "Epoch 23/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9899\n",
            "Epoch 00023: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 11s 68ms/step - loss: 0.0267 - acc: 0.9896 - val_loss: 0.2505 - val_acc: 0.9547\n",
            "Epoch 24/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0203 - acc: 0.9929\n",
            "Epoch 00024: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 69ms/step - loss: 0.0203 - acc: 0.9929 - val_loss: 0.2576 - val_acc: 0.9547\n",
            "Epoch 25/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9943\n",
            "Epoch 00025: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 69ms/step - loss: 0.0215 - acc: 0.9944 - val_loss: 0.2630 - val_acc: 0.9493\n",
            "Epoch 26/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9926\n",
            "Epoch 00026: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 69ms/step - loss: 0.0213 - acc: 0.9926 - val_loss: 0.2729 - val_acc: 0.9520\n",
            "Epoch 27/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9940\n",
            "Epoch 00027: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 70ms/step - loss: 0.0181 - acc: 0.9941 - val_loss: 0.2743 - val_acc: 0.9467\n",
            "Epoch 28/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9937\n",
            "Epoch 00028: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 74ms/step - loss: 0.0200 - acc: 0.9938 - val_loss: 0.2712 - val_acc: 0.9511\n",
            "Epoch 29/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9929\n",
            "Epoch 00029: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 13s 74ms/step - loss: 0.0204 - acc: 0.9929 - val_loss: 0.2729 - val_acc: 0.9502\n",
            "Epoch 30/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0161 - acc: 0.9959\n",
            "Epoch 00030: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.2788 - val_acc: 0.9502\n",
            "Epoch 31/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9911\n",
            "Epoch 00031: val_loss did not improve from 0.20930\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0209 - acc: 0.9911 - val_loss: 0.2787 - val_acc: 0.9511\n",
            "Epoch 32/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0167 - acc: 0.9961\n",
            "Epoch 00032: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0167 - acc: 0.9961 - val_loss: 0.2788 - val_acc: 0.9502\n",
            "Epoch 33/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9955\n",
            "Epoch 00033: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0164 - acc: 0.9956 - val_loss: 0.2789 - val_acc: 0.9502\n",
            "Epoch 34/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0190 - acc: 0.9944\n",
            "Epoch 00034: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0190 - acc: 0.9944 - val_loss: 0.2795 - val_acc: 0.9502\n",
            "Epoch 35/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9952\n",
            "Epoch 00035: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0168 - acc: 0.9953 - val_loss: 0.2797 - val_acc: 0.9502\n",
            "Epoch 36/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0147 - acc: 0.9961\n",
            "Epoch 00036: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0147 - acc: 0.9961 - val_loss: 0.2798 - val_acc: 0.9502\n",
            "Epoch 37/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0163 - acc: 0.9947\n",
            "Epoch 00037: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0163 - acc: 0.9947 - val_loss: 0.2804 - val_acc: 0.9502\n",
            "Epoch 38/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0135 - acc: 0.9964\n",
            "Epoch 00038: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0135 - acc: 0.9964 - val_loss: 0.2807 - val_acc: 0.9493\n",
            "Epoch 39/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9967\n",
            "Epoch 00039: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0154 - acc: 0.9967 - val_loss: 0.2805 - val_acc: 0.9502\n",
            "Epoch 40/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0189 - acc: 0.9944\n",
            "Epoch 00040: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0189 - acc: 0.9944 - val_loss: 0.2803 - val_acc: 0.9502\n",
            "Epoch 41/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9952\n",
            "Epoch 00041: val_loss did not improve from 0.20930\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0149 - acc: 0.9953 - val_loss: 0.2806 - val_acc: 0.9502\n",
            "Epoch 42/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0155 - acc: 0.9959\n",
            "Epoch 00042: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 70ms/step - loss: 0.0155 - acc: 0.9959 - val_loss: 0.2806 - val_acc: 0.9502\n",
            "Epoch 43/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0136 - acc: 0.9959\n",
            "Epoch 00043: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0136 - acc: 0.9959 - val_loss: 0.2807 - val_acc: 0.9502\n",
            "Epoch 44/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9955\n",
            "Epoch 00044: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0169 - acc: 0.9956 - val_loss: 0.2806 - val_acc: 0.9502\n",
            "Epoch 45/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0150 - acc: 0.9964\n",
            "Epoch 00045: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0150 - acc: 0.9964 - val_loss: 0.2807 - val_acc: 0.9502\n",
            "Epoch 46/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9952\n",
            "Epoch 00046: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0142 - acc: 0.9953 - val_loss: 0.2808 - val_acc: 0.9502\n",
            "Epoch 47/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9926\n",
            "Epoch 00047: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 70ms/step - loss: 0.0173 - acc: 0.9926 - val_loss: 0.2808 - val_acc: 0.9502\n",
            "Epoch 48/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9958\n",
            "Epoch 00048: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.2808 - val_acc: 0.9502\n",
            "Epoch 49/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0175 - acc: 0.9950\n",
            "Epoch 00049: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0175 - acc: 0.9950 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 50/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0145 - acc: 0.9973\n",
            "Epoch 00050: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0145 - acc: 0.9973 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 51/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9961\n",
            "Epoch 00051: val_loss did not improve from 0.20930\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0136 - acc: 0.9961 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 52/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0159 - acc: 0.9959\n",
            "Epoch 00052: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0159 - acc: 0.9959 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 53/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9955\n",
            "Epoch 00053: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0149 - acc: 0.9953 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 54/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0139 - acc: 0.9976\n",
            "Epoch 00054: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0139 - acc: 0.9976 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 55/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0136 - acc: 0.9959\n",
            "Epoch 00055: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0136 - acc: 0.9959 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 56/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0159 - acc: 0.9956\n",
            "Epoch 00056: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0159 - acc: 0.9956 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 57/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9964\n",
            "Epoch 00057: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0153 - acc: 0.9961 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 58/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9952\n",
            "Epoch 00058: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 74ms/step - loss: 0.0169 - acc: 0.9953 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 59/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0159 - acc: 0.9964\n",
            "Epoch 00059: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0159 - acc: 0.9964 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 60/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0160 - acc: 0.9944\n",
            "Epoch 00060: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0160 - acc: 0.9944 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 61/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9964\n",
            "Epoch 00061: val_loss did not improve from 0.20930\n",
            "\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0146 - acc: 0.9961 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 62/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0128 - acc: 0.9970\n",
            "Epoch 00062: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0128 - acc: 0.9970 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 63/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9949\n",
            "Epoch 00063: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0172 - acc: 0.9950 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 64/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0129 - acc: 0.9973\n",
            "Epoch 00064: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0129 - acc: 0.9973 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 65/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9958\n",
            "Epoch 00065: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 74ms/step - loss: 0.0156 - acc: 0.9959 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 66/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9961\n",
            "Epoch 00066: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0135 - acc: 0.9961 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 67/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0147 - acc: 0.9950\n",
            "Epoch 00067: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0147 - acc: 0.9950 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 68/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0133 - acc: 0.9967\n",
            "Epoch 00068: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 74ms/step - loss: 0.0133 - acc: 0.9967 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 69/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0151 - acc: 0.9941\n",
            "Epoch 00069: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0151 - acc: 0.9941 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 70/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9955\n",
            "Epoch 00070: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0151 - acc: 0.9956 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 71/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0146 - acc: 0.9953\n",
            "Epoch 00071: val_loss did not improve from 0.20930\n",
            "\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0146 - acc: 0.9953 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 72/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9967\n",
            "Epoch 00072: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0146 - acc: 0.9967 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 73/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9955\n",
            "Epoch 00073: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 70ms/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 74/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0141 - acc: 0.9961\n",
            "Epoch 00074: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0141 - acc: 0.9961 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 75/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9964\n",
            "Epoch 00075: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 70ms/step - loss: 0.0134 - acc: 0.9964 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 76/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9949\n",
            "Epoch 00076: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 70ms/step - loss: 0.0155 - acc: 0.9950 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 77/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0148 - acc: 0.9964\n",
            "Epoch 00077: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 70ms/step - loss: 0.0148 - acc: 0.9964 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 78/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0128 - acc: 0.9956\n",
            "Epoch 00078: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 70ms/step - loss: 0.0128 - acc: 0.9956 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 79/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0146 - acc: 0.9950\n",
            "Epoch 00079: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0146 - acc: 0.9950 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 80/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0154 - acc: 0.9956\n",
            "Epoch 00080: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 73ms/step - loss: 0.0154 - acc: 0.9956 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 81/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0159 - acc: 0.9967\n",
            "Epoch 00081: val_loss did not improve from 0.20930\n",
            "\n",
            "Epoch 00081: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0159 - acc: 0.9967 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 82/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0140 - acc: 0.9956\n",
            "Epoch 00082: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0140 - acc: 0.9956 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 83/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0164 - acc: 0.9947\n",
            "Epoch 00083: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0164 - acc: 0.9947 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 84/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9967\n",
            "Epoch 00084: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0163 - acc: 0.9967 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 85/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0176 - acc: 0.9944\n",
            "Epoch 00085: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0176 - acc: 0.9944 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 86/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0153 - acc: 0.9956\n",
            "Epoch 00086: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0153 - acc: 0.9956 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 87/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9958\n",
            "Epoch 00087: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 70ms/step - loss: 0.0155 - acc: 0.9956 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 88/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9952\n",
            "Epoch 00088: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0154 - acc: 0.9953 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 89/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0177 - acc: 0.9944\n",
            "Epoch 00089: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0177 - acc: 0.9944 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 90/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0172 - acc: 0.9950\n",
            "Epoch 00090: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0172 - acc: 0.9950 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 91/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0148 - acc: 0.9964\n",
            "Epoch 00091: val_loss did not improve from 0.20930\n",
            "\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0148 - acc: 0.9964 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 92/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0134 - acc: 0.9959\n",
            "Epoch 00092: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0134 - acc: 0.9959 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 93/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0133 - acc: 0.9970\n",
            "Epoch 00093: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 74ms/step - loss: 0.0133 - acc: 0.9970 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 94/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0149 - acc: 0.9956\n",
            "Epoch 00094: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 13s 74ms/step - loss: 0.0149 - acc: 0.9956 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 95/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0144 - acc: 0.9961\n",
            "Epoch 00095: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0144 - acc: 0.9961 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 96/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9949\n",
            "Epoch 00096: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0150 - acc: 0.9950 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 97/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0150 - acc: 0.9959\n",
            "Epoch 00097: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0150 - acc: 0.9959 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 98/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9961\n",
            "Epoch 00098: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0156 - acc: 0.9961 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 99/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0139 - acc: 0.9953\n",
            "Epoch 00099: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 72ms/step - loss: 0.0139 - acc: 0.9953 - val_loss: 0.2809 - val_acc: 0.9502\n",
            "Epoch 100/100\n",
            "168/169 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9955\n",
            "Epoch 00100: val_loss did not improve from 0.20930\n",
            "169/169 [==============================] - 12s 71ms/step - loss: 0.0142 - acc: 0.9956 - val_loss: 0.2809 - val_acc: 0.9502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppzHiTkLFbxq",
        "outputId": "7a927c81-1da1-4a63-c60b-65ceccf793a8"
      },
      "source": [
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.2809215188026428\n",
            "Test accuracy: 0.9502221941947937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "O9NCt5MkFbxq",
        "outputId": "974ef48f-c6ed-443d-fd5f-eba8b4a27b46"
      },
      "source": [
        "plt.plot(history.history['acc'],label=\"accuracy\")\n",
        "plt.plot(history.history['val_acc'],label=\"val_accuracy\")\n",
        "plt.plot(history.history['loss'],label=\"loss\")\n",
        "plt.plot(history.history['val_loss'],label=\"val_loss\")\n",
        "plt.legend()\n",
        "plt.title(\"FR Feature Selection\")\n",
        "plt.xlabel('epoch')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'epoch')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rDuXxTe4giA"
      },
      "source": [
        "## Save the model/weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cQF507YFbxr",
        "outputId": "c33e1be0-17ad-45b0-dbcd-565b2e74548d"
      },
      "source": [
        "# JSON JSON\n",
        "# serialize model to json\n",
        "json_model = model.to_json()\n",
        "\n",
        "# save the model architecture to JSON file\n",
        "with open('{}/{}.model.json'.format(output_dir, model_name), 'w') as json_file:\n",
        "    json_file.write(json_model)\n",
        "\n",
        "\n",
        "# YAML YAML\n",
        "# serialize model to YAML\n",
        "model_yaml = model.to_yaml()\n",
        "\n",
        "# save the model architecture to YAML file\n",
        "with open(\"{}/{}.model.yaml\".format(output_dir, model_name), \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "\n",
        "\n",
        "# WEIGHTS HDF5\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"{}/{}.model.h5\".format(output_dir,model_name))\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnvbuBhg2Nz7",
        "outputId": "9baa8df5-f8af-4cfd-8e08-2e19a2018091"
      },
      "source": [
        "# Open the handle\n",
        "json_file = open('{}/{}.model.json'.format(output_dir, model_name), 'r')\n",
        "\n",
        "# load json and create model\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model.load_weights('{}/{}.model.h5'.format(output_dir, model_name))\n",
        "print(\"Loaded model from disk\")\n",
        "# loaded_model_json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE3Nx-TvvuH4",
        "outputId": "992ab4a4-d0e2-4e72-b84d-19b7caa743dd"
      },
      "source": [
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='categorical_crossentropy', optimizer='sgd', \n",
        "                     metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.31975093483924866\n",
            "Test accuracy: 0.9528889060020447\n",
            "accuracy: 95.29%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TotWnrs86sVp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qYKC2dX6tDe"
      },
      "source": [
        "# ConvNN(RF) - top 518"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbow5tSi6tDe"
      },
      "source": [
        "## Train/Test split    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUecTn5M6tDf"
      },
      "source": [
        "# from keras.utils import to_categorical\n",
        "# outcome = encode(outcome['Project_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "kxaEe6UK6tDg",
        "outputId": "7a56bca4-13a0-4bbd-f119-b5cd9871e5c5"
      },
      "source": [
        "feature_sort_selected = feature_sort[feature_sort['importance'] > 0.0005]\n",
        "feature_sort_selected"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10258</th>\n",
              "      <td>SLC45A3</td>\n",
              "      <td>0.003607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36533</th>\n",
              "      <td>CTD-2182N23.1</td>\n",
              "      <td>0.003576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29180</th>\n",
              "      <td>AC012123.1</td>\n",
              "      <td>0.003306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6445</th>\n",
              "      <td>NAPSA</td>\n",
              "      <td>0.002897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12521</th>\n",
              "      <td>C8orf46</td>\n",
              "      <td>0.002802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16420</th>\n",
              "      <td>TMPRSS6</td>\n",
              "      <td>0.000504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30704</th>\n",
              "      <td>LINC00271</td>\n",
              "      <td>0.000504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13539</th>\n",
              "      <td>PTPRM</td>\n",
              "      <td>0.000504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5141</th>\n",
              "      <td>NCAPH</td>\n",
              "      <td>0.000504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8225</th>\n",
              "      <td>SLC6A3</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>518 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                name  importance\n",
              "10258        SLC45A3    0.003607\n",
              "36533  CTD-2182N23.1    0.003576\n",
              "29180     AC012123.1    0.003306\n",
              "6445           NAPSA    0.002897\n",
              "12521        C8orf46    0.002802\n",
              "...              ...         ...\n",
              "16420        TMPRSS6    0.000504\n",
              "30704      LINC00271    0.000504\n",
              "13539          PTPRM    0.000504\n",
              "5141           NCAPH    0.000504\n",
              "8225          SLC6A3    0.000500\n",
              "\n",
              "[518 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mju0x7ig6tDg"
      },
      "source": [
        "TC1data15_selected = TC1data15.loc[:,feature_sort_selected['name']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6yffhkU6tDg"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(TC1data15_selected, \n",
        "                                                    outcome, \n",
        "                                                    train_size=0.75, \n",
        "                                                    test_size=0.25, \n",
        "                                                    random_state=123, \n",
        "                                                    stratify = outcome)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "Sxjv7l636tDh",
        "outputId": "10db4656-1efe-4ea6-d003-1a72f9bb80e2"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SLC45A3</th>\n",
              "      <th>CTD-2182N23.1</th>\n",
              "      <th>AC012123.1</th>\n",
              "      <th>NAPSA</th>\n",
              "      <th>C8orf46</th>\n",
              "      <th>AL161626.1</th>\n",
              "      <th>RP11-264B14.2</th>\n",
              "      <th>FP236383.9</th>\n",
              "      <th>SOX17</th>\n",
              "      <th>ACSM2B</th>\n",
              "      <th>PAX8</th>\n",
              "      <th>U47924.27</th>\n",
              "      <th>PSD2</th>\n",
              "      <th>FP671120.6</th>\n",
              "      <th>TRPS1</th>\n",
              "      <th>NKX2-1-AS1</th>\n",
              "      <th>CHRNA2</th>\n",
              "      <th>TSHR</th>\n",
              "      <th>GAL3ST1</th>\n",
              "      <th>SLC28A1</th>\n",
              "      <th>APOC3</th>\n",
              "      <th>Y_RNA.262</th>\n",
              "      <th>C8B</th>\n",
              "      <th>SNRPGP10</th>\n",
              "      <th>RP11-25I15.1</th>\n",
              "      <th>SLC17A3</th>\n",
              "      <th>SLC2A2</th>\n",
              "      <th>SLC39A5</th>\n",
              "      <th>TM4SF5</th>\n",
              "      <th>PCA3</th>\n",
              "      <th>RMST</th>\n",
              "      <th>ATP8A2P1</th>\n",
              "      <th>MGAT4C</th>\n",
              "      <th>HEPH</th>\n",
              "      <th>ORM2</th>\n",
              "      <th>PLG</th>\n",
              "      <th>CRYGN</th>\n",
              "      <th>RP11-53M11.5</th>\n",
              "      <th>TRABD2A</th>\n",
              "      <th>IYD</th>\n",
              "      <th>...</th>\n",
              "      <th>KRT16P2</th>\n",
              "      <th>CDC25B</th>\n",
              "      <th>LINC00958</th>\n",
              "      <th>FGF18</th>\n",
              "      <th>CHST13</th>\n",
              "      <th>CEP55</th>\n",
              "      <th>FABP1</th>\n",
              "      <th>WNK4</th>\n",
              "      <th>PAGE4</th>\n",
              "      <th>RAB6C-AS1</th>\n",
              "      <th>PAM</th>\n",
              "      <th>RP11-599B13.3</th>\n",
              "      <th>PLA2G4F</th>\n",
              "      <th>HOXA9</th>\n",
              "      <th>VGLL1</th>\n",
              "      <th>RP5-967N21.11</th>\n",
              "      <th>MARC2</th>\n",
              "      <th>ZNF541</th>\n",
              "      <th>TRIM15</th>\n",
              "      <th>CKAP2L</th>\n",
              "      <th>HOXC9</th>\n",
              "      <th>BIRC5</th>\n",
              "      <th>RGAG4</th>\n",
              "      <th>LIPH</th>\n",
              "      <th>RP11-285E23.2</th>\n",
              "      <th>BDH2</th>\n",
              "      <th>G6PC3</th>\n",
              "      <th>AR</th>\n",
              "      <th>SH3RF2</th>\n",
              "      <th>SCGB2A1</th>\n",
              "      <th>SPP2</th>\n",
              "      <th>LA16c-312E8.4</th>\n",
              "      <th>PKP3</th>\n",
              "      <th>CITED1</th>\n",
              "      <th>CTB-36O1.4</th>\n",
              "      <th>TMPRSS6</th>\n",
              "      <th>LINC00271</th>\n",
              "      <th>PTPRM</th>\n",
              "      <th>NCAPH</th>\n",
              "      <th>SLC6A3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>849</th>\n",
              "      <td>0.717435</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.672119</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.424186</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.170832</td>\n",
              "      <td>1.493830</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.094042</td>\n",
              "      <td>1.026745</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.924745</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079015</td>\n",
              "      <td>0.615134</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.229850</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.752871</td>\n",
              "      <td>1.765075</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.184754</td>\n",
              "      <td>0.529635</td>\n",
              "      <td>...</td>\n",
              "      <td>0.846334</td>\n",
              "      <td>1.931741</td>\n",
              "      <td>0.286145</td>\n",
              "      <td>0.092101</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.670893</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.114156</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.763569</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.517548</td>\n",
              "      <td>0.585738</td>\n",
              "      <td>1.700248</td>\n",
              "      <td>0.380702</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.109185</td>\n",
              "      <td>0.511817</td>\n",
              "      <td>0.614766</td>\n",
              "      <td>0.554460</td>\n",
              "      <td>2.062177</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.026451</td>\n",
              "      <td>1.733515</td>\n",
              "      <td>0.750422</td>\n",
              "      <td>0.128029</td>\n",
              "      <td>0.451394</td>\n",
              "      <td>1.214561</td>\n",
              "      <td>0.606132</td>\n",
              "      <td>1.721816</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.691488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.509945</td>\n",
              "      <td>0.391520</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1050</th>\n",
              "      <td>0.901859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.732119</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.257574</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.420687</td>\n",
              "      <td>0.861244</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.566138</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.137554</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.473346</td>\n",
              "      <td>0.452157</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.780366</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.993561</td>\n",
              "      <td>2.947773</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.392027</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.834421</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.052220</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.762665</td>\n",
              "      <td>0.252325</td>\n",
              "      <td>0.831562</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.379318</td>\n",
              "      <td>0.131957</td>\n",
              "      <td>2.203330</td>\n",
              "      <td>0.128463</td>\n",
              "      <td>0.679752</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.988780</td>\n",
              "      <td>1.633399</td>\n",
              "      <td>0.429567</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.941200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.150359</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.178443</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.931182</td>\n",
              "      <td>1.476843</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>1.169069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.690877</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365155</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041478</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.848804</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.157242</td>\n",
              "      <td>0.557529</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.914976</td>\n",
              "      <td>0.622720</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.659029</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.040108</td>\n",
              "      <td>2.043236</td>\n",
              "      <td>1.027411</td>\n",
              "      <td>0.348096</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.508080</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.782595</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.241283</td>\n",
              "      <td>0.925288</td>\n",
              "      <td>0.079355</td>\n",
              "      <td>1.113506</td>\n",
              "      <td>0.939554</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.855996</td>\n",
              "      <td>1.116087</td>\n",
              "      <td>1.854245</td>\n",
              "      <td>0.743807</td>\n",
              "      <td>0.778923</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.502086</td>\n",
              "      <td>1.585139</td>\n",
              "      <td>0.031703</td>\n",
              "      <td>1.356941</td>\n",
              "      <td>0.103162</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.111933</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046214</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.597420</td>\n",
              "      <td>1.288918</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4447</th>\n",
              "      <td>1.001501</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.393282</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.284720</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.510884</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039429</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.537337</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.589360</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.351233</td>\n",
              "      <td>0.772481</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.318423</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.296706</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.612812</td>\n",
              "      <td>1.256689</td>\n",
              "      <td>2.039085</td>\n",
              "      <td>1.128795</td>\n",
              "      <td>0.980197</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.980573</td>\n",
              "      <td>0.572466</td>\n",
              "      <td>1.411829</td>\n",
              "      <td>0.032112</td>\n",
              "      <td>0.304901</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.173131</td>\n",
              "      <td>1.500076</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.868184</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.288972</td>\n",
              "      <td>1.135975</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0.429030</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.698742</td>\n",
              "      <td>0.094371</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.670652</td>\n",
              "      <td>0.708519</td>\n",
              "      <td>2.400472</td>\n",
              "      <td>2.316836</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.962383</td>\n",
              "      <td>1.967081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.948668</td>\n",
              "      <td>1.550796</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365934</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.092616</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.604855</td>\n",
              "      <td>1.268017</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.480250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.133977</td>\n",
              "      <td>0.475185</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.915313</td>\n",
              "      <td>1.262870</td>\n",
              "      <td>0.537637</td>\n",
              "      <td>0.303451</td>\n",
              "      <td>1.644392</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.338379</td>\n",
              "      <td>0.107885</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.815124</td>\n",
              "      <td>1.768175</td>\n",
              "      <td>0.958802</td>\n",
              "      <td>0.681281</td>\n",
              "      <td>0.215052</td>\n",
              "      <td>0.155523</td>\n",
              "      <td>1.239451</td>\n",
              "      <td>0.837586</td>\n",
              "      <td>1.902639</td>\n",
              "      <td>0.762819</td>\n",
              "      <td>0.722267</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.923130</td>\n",
              "      <td>1.998748</td>\n",
              "      <td>1.426661</td>\n",
              "      <td>1.212653</td>\n",
              "      <td>2.653249</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.842547</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.639184</td>\n",
              "      <td>0.005554</td>\n",
              "      <td>0.595043</td>\n",
              "      <td>0.843370</td>\n",
              "      <td>1.594739</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3495</th>\n",
              "      <td>1.558939</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.636459</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.052139</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.436981</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.659693</td>\n",
              "      <td>0.337474</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.196452</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.168646</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.808359</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.262081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.683020</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.370337</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.944003</td>\n",
              "      <td>1.374017</td>\n",
              "      <td>2.065453</td>\n",
              "      <td>1.173358</td>\n",
              "      <td>0.923740</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.823205</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.568157</td>\n",
              "      <td>0.629204</td>\n",
              "      <td>1.630440</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.398149</td>\n",
              "      <td>1.648088</td>\n",
              "      <td>0.685592</td>\n",
              "      <td>1.037978</td>\n",
              "      <td>0.198128</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.960780</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.593477</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.192910</td>\n",
              "      <td>1.280170</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1737</th>\n",
              "      <td>0.508131</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.234141</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.229859</td>\n",
              "      <td>1.537464</td>\n",
              "      <td>1.877514</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.735723</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.093082</td>\n",
              "      <td>1.849123</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.315884</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.722</td>\n",
              "      <td>1.62879</td>\n",
              "      <td>1.310074</td>\n",
              "      <td>1.645405</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.199691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.155142</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.403400</td>\n",
              "      <td>0.892547</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.020014</td>\n",
              "      <td>0.478775</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.969311</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.742140</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.196519</td>\n",
              "      <td>1.503263</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.331410</td>\n",
              "      <td>0.037933</td>\n",
              "      <td>0.842290</td>\n",
              "      <td>0.459767</td>\n",
              "      <td>1.124473</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.668569</td>\n",
              "      <td>1.586301</td>\n",
              "      <td>0.811374</td>\n",
              "      <td>1.126067</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.780736</td>\n",
              "      <td>0.404002</td>\n",
              "      <td>2.426038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2943</th>\n",
              "      <td>0.794707</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.148528</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.383600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.406436</td>\n",
              "      <td>1.454994</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.320577</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.268603</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.417348</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.399102</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.372178</td>\n",
              "      <td>1.661578</td>\n",
              "      <td>1.586461</td>\n",
              "      <td>0.775716</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.479605</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.976729</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.365265</td>\n",
              "      <td>1.284841</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.151766</td>\n",
              "      <td>0.990598</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.799577</td>\n",
              "      <td>0.939416</td>\n",
              "      <td>1.363948</td>\n",
              "      <td>1.050524</td>\n",
              "      <td>0.363104</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.526705</td>\n",
              "      <td>1.497145</td>\n",
              "      <td>0.414221</td>\n",
              "      <td>1.244070</td>\n",
              "      <td>0.175014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.962697</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.003201</td>\n",
              "      <td>1.015730</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1573</th>\n",
              "      <td>0.891176</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.707764</td>\n",
              "      <td>0.152280</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.282959</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.620480</td>\n",
              "      <td>1.605914</td>\n",
              "      <td>0.051819</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.200227</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.238608</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.973891</td>\n",
              "      <td>0.225703</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.158866</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.991760</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.702341</td>\n",
              "      <td>1.196331</td>\n",
              "      <td>2.532338</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.737392</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.160977</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.411945</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.560338</td>\n",
              "      <td>1.627724</td>\n",
              "      <td>1.553244</td>\n",
              "      <td>0.032711</td>\n",
              "      <td>0.541690</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.862147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.846555</td>\n",
              "      <td>0.985193</td>\n",
              "      <td>0.168343</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.472550</td>\n",
              "      <td>1.893103</td>\n",
              "      <td>0.738226</td>\n",
              "      <td>0.834792</td>\n",
              "      <td>1.252688</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.837683</td>\n",
              "      <td>0.266197</td>\n",
              "      <td>0.718618</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.520583</td>\n",
              "      <td>1.576877</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3192</th>\n",
              "      <td>0.440866</td>\n",
              "      <td>3.215473</td>\n",
              "      <td>0.649271</td>\n",
              "      <td>0.455298</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.852791</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.665235</td>\n",
              "      <td>1.730446</td>\n",
              "      <td>0.292689</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.725373</td>\n",
              "      <td>1.328306</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.821487</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.222551</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055060</td>\n",
              "      <td>0.227691</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.75023</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.258463</td>\n",
              "      <td>0.762914</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.617423</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.566856</td>\n",
              "      <td>1.581944</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.759582</td>\n",
              "      <td>0.766281</td>\n",
              "      <td>0.571661</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.214781</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.310508</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.329269</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.894607</td>\n",
              "      <td>0.466063</td>\n",
              "      <td>1.266158</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064630</td>\n",
              "      <td>1.799542</td>\n",
              "      <td>1.975594</td>\n",
              "      <td>0.725033</td>\n",
              "      <td>1.784635</td>\n",
              "      <td>1.902544</td>\n",
              "      <td>0.380063</td>\n",
              "      <td>0.713327</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.628977</td>\n",
              "      <td>2.165908</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.973136</td>\n",
              "      <td>0.490505</td>\n",
              "      <td>2.160312</td>\n",
              "      <td>0.259610</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3375 rows × 518 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       SLC45A3  CTD-2182N23.1  AC012123.1  ...     PTPRM     NCAPH    SLC6A3\n",
              "849   0.717435       0.000000    0.000000  ...  1.509945  0.391520  0.000000\n",
              "1050  0.901859       0.000000    0.000000  ...  0.931182  1.476843  0.000000\n",
              "835   1.169069       0.000000    0.000000  ...  0.597420  1.288918  0.000000\n",
              "4447  1.001501       0.000000    0.000000  ...  1.288972  1.135975  0.000000\n",
              "53    0.429030       0.000000    0.698742  ...  0.843370  1.594739  0.000000\n",
              "...        ...            ...         ...  ...       ...       ...       ...\n",
              "3495  1.558939       0.000000    0.000000  ...  1.192910  1.280170  0.000000\n",
              "1737  0.508131       0.000000    0.000000  ...  1.780736  0.404002  2.426038\n",
              "2943  0.794707       0.000000    0.000000  ...  1.003201  1.015730  0.000000\n",
              "1573  0.891176       0.000000    0.707764  ...  1.520583  1.576877  0.000000\n",
              "3192  0.440866       3.215473    0.649271  ...  2.160312  0.259610  0.000000\n",
              "\n",
              "[3375 rows x 518 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPz4mZIT6tDh"
      },
      "source": [
        "## CONV1D "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVioxP5B6tDh"
      },
      "source": [
        "# parameters  \n",
        "activation='relu'\n",
        "batch_size=20\n",
        "# Number of sites\n",
        "classes=15\n",
        "drop = 0.1\n",
        "feature_subsample = 0\n",
        "loss='categorical_crossentropy'\n",
        "# metrics='accuracy'\n",
        "out_act='softmax'\n",
        "pool=[1, 10]\n",
        "# optimizer='sgd'\n",
        "shuffle = False \n",
        "epochs=100\n",
        "\n",
        "optimizer = optimizers.SGD(lr=0.1)\n",
        "metrics = ['acc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlf3PDbs6tDi"
      },
      "source": [
        "x_train_len = X_train.shape[1]   \n",
        "\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "filters = 128 \n",
        "filter_len = 20 \n",
        "stride = 1 \n",
        "\n",
        "# inside pool_list loop\n",
        "pool_list = [1,10]\n",
        "\n",
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LkBfPFt6tDi",
        "outputId": "033ed9c1-021c-4727-8a8e-4a337d896bc4"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# model.add  CONV1D\n",
        "model.add(Conv1D(filters = filters, \n",
        "                 kernel_size = filter_len, \n",
        "                 strides = stride, \n",
        "                 padding='valid', \n",
        "                 input_shape=(x_train_len, 1)))\n",
        "\n",
        "# Activation\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# MaxPooling\n",
        "model.add(MaxPooling1D(pool_size = 1))\n",
        "\n",
        "filters = 128\n",
        "filter_len = 10 \n",
        "stride = 1 \n",
        "# Conv1D\n",
        "model.add(Conv1D(filters=filters, \n",
        "                 kernel_size=filter_len, \n",
        "                 strides=stride, \n",
        "                 padding='valid'))\n",
        "# Activation\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# MaxPooling\n",
        "model.add(MaxPooling1D(pool_size = 10))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(200))\n",
        "\n",
        "# activation \n",
        "# model.add(Activation('relu')) # SR\n",
        "model.add(Activation(activation))\n",
        "#dropout\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(20))\n",
        "# activation\n",
        "# model.add(Activation('relu')) # SR\n",
        "model.add(Activation(activation))\n",
        "\n",
        "#dropout\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(15))\n",
        "model.add(Activation(out_act))\n",
        "\n",
        "model.compile( loss= loss, \n",
        "              optimizer = optimizer, \n",
        "              metrics = metrics )\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 499, 128)          2688      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 499, 128)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 499, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 490, 128)          163968    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 490, 128)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 49, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               1254600   \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                4020      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 15)                315       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 1,425,591\n",
            "Trainable params: 1,425,591\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuVv6btS6tDi"
      },
      "source": [
        "# save\n",
        "# save = '/content/drive/My Drive/FNL_TC1/'\n",
        "output_dir = \"/content/drive/My Drive/FNL_TC1/Model\"\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "model_name = 'tc1_rf'\n",
        "path = '{}/{}.autosave.model.h5'.format(output_dir, model_name)\n",
        "checkpointer = ModelCheckpoint(filepath=path,\n",
        "                               verbose=1,\n",
        "                               save_weights_only=True,\n",
        "                               save_best_only=True)\n",
        "\n",
        "csv_logger = CSVLogger('{}/training.log'.format(output_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfBGZeDN6tDi"
      },
      "source": [
        "# SR: change epsilon to min_delta\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
        "                              factor=0.1, \n",
        "                              patience=10, \n",
        "                              verbose=1, mode='auto', \n",
        "                              min_delta=0.0001, \n",
        "                              cooldown=0, \n",
        "                              min_lr=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3PnJuAa6tDj",
        "outputId": "41127ef6-30c7-4553-fb24-0cfe0f496a71"
      },
      "source": [
        "# batch_size = 20 \n",
        "history = model.fit(X_train, Y_train, batch_size=batch_size, \n",
        "                    epochs=epochs, verbose=1, validation_data=(X_test, Y_test), \n",
        "                    callbacks = [checkpointer, csv_logger, reduce_lr])\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 1.2712 - acc: 0.5748\n",
            "Epoch 00001: val_loss improved from inf to 0.40486, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 44s 263ms/step - loss: 1.2712 - acc: 0.5748 - val_loss: 0.4049 - val_acc: 0.8640\n",
            "Epoch 2/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.2904 - acc: 0.9061\n",
            "Epoch 00002: val_loss improved from 0.40486 to 0.25729, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.2904 - acc: 0.9061 - val_loss: 0.2573 - val_acc: 0.9244\n",
            "Epoch 3/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1880 - acc: 0.9437\n",
            "Epoch 00003: val_loss improved from 0.25729 to 0.16209, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.1880 - acc: 0.9437 - val_loss: 0.1621 - val_acc: 0.9529\n",
            "Epoch 4/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1419 - acc: 0.9576\n",
            "Epoch 00004: val_loss improved from 0.16209 to 0.14869, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.1419 - acc: 0.9576 - val_loss: 0.1487 - val_acc: 0.9556\n",
            "Epoch 5/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1121 - acc: 0.9692\n",
            "Epoch 00005: val_loss did not improve from 0.14869\n",
            "169/169 [==============================] - 44s 259ms/step - loss: 0.1121 - acc: 0.9692 - val_loss: 0.1621 - val_acc: 0.9556\n",
            "Epoch 6/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0985 - acc: 0.9692\n",
            "Epoch 00006: val_loss did not improve from 0.14869\n",
            "169/169 [==============================] - 44s 259ms/step - loss: 0.0985 - acc: 0.9692 - val_loss: 0.1787 - val_acc: 0.9529\n",
            "Epoch 7/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0812 - acc: 0.9760\n",
            "Epoch 00007: val_loss improved from 0.14869 to 0.14037, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0812 - acc: 0.9760 - val_loss: 0.1404 - val_acc: 0.9671\n",
            "Epoch 8/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0735 - acc: 0.9772\n",
            "Epoch 00008: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0735 - acc: 0.9772 - val_loss: 0.2113 - val_acc: 0.9440\n",
            "Epoch 9/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0579 - acc: 0.9804\n",
            "Epoch 00009: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 259ms/step - loss: 0.0579 - acc: 0.9804 - val_loss: 0.1775 - val_acc: 0.9520\n",
            "Epoch 10/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0582 - acc: 0.9825\n",
            "Epoch 00010: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 259ms/step - loss: 0.0582 - acc: 0.9825 - val_loss: 0.1577 - val_acc: 0.9618\n",
            "Epoch 11/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0574 - acc: 0.9837\n",
            "Epoch 00011: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 259ms/step - loss: 0.0574 - acc: 0.9837 - val_loss: 0.2109 - val_acc: 0.9529\n",
            "Epoch 12/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0504 - acc: 0.9873\n",
            "Epoch 00012: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 263ms/step - loss: 0.0504 - acc: 0.9873 - val_loss: 0.1752 - val_acc: 0.9653\n",
            "Epoch 13/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0269 - acc: 0.9914\n",
            "Epoch 00013: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0269 - acc: 0.9914 - val_loss: 0.1774 - val_acc: 0.9627\n",
            "Epoch 14/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0317 - acc: 0.9914\n",
            "Epoch 00014: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0317 - acc: 0.9914 - val_loss: 0.1764 - val_acc: 0.9644\n",
            "Epoch 15/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0246 - acc: 0.9917\n",
            "Epoch 00015: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0246 - acc: 0.9917 - val_loss: 0.2057 - val_acc: 0.9573\n",
            "Epoch 16/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0285 - acc: 0.9905\n",
            "Epoch 00016: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0285 - acc: 0.9905 - val_loss: 0.2107 - val_acc: 0.9582\n",
            "Epoch 17/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0134 - acc: 0.9956\n",
            "Epoch 00017: val_loss did not improve from 0.14037\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0134 - acc: 0.9956 - val_loss: 0.2292 - val_acc: 0.9582\n",
            "Epoch 18/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0123 - acc: 0.9950\n",
            "Epoch 00018: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0123 - acc: 0.9950 - val_loss: 0.2096 - val_acc: 0.9636\n",
            "Epoch 19/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0096 - acc: 0.9976\n",
            "Epoch 00019: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 259ms/step - loss: 0.0096 - acc: 0.9976 - val_loss: 0.2131 - val_acc: 0.9618\n",
            "Epoch 20/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0054 - acc: 0.9988\n",
            "Epoch 00020: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0054 - acc: 0.9988 - val_loss: 0.2166 - val_acc: 0.9627\n",
            "Epoch 21/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0084 - acc: 0.9982\n",
            "Epoch 00021: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0084 - acc: 0.9982 - val_loss: 0.2215 - val_acc: 0.9627\n",
            "Epoch 22/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0056 - acc: 0.9985\n",
            "Epoch 00022: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 259ms/step - loss: 0.0056 - acc: 0.9985 - val_loss: 0.2216 - val_acc: 0.9627\n",
            "Epoch 23/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9988\n",
            "Epoch 00023: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 259ms/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.2234 - val_acc: 0.9636\n",
            "Epoch 24/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0068 - acc: 0.9976\n",
            "Epoch 00024: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0068 - acc: 0.9976 - val_loss: 0.2219 - val_acc: 0.9636\n",
            "Epoch 25/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0050 - acc: 0.9991\n",
            "Epoch 00025: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 258ms/step - loss: 0.0050 - acc: 0.9991 - val_loss: 0.2252 - val_acc: 0.9618\n",
            "Epoch 26/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0064 - acc: 0.9988\n",
            "Epoch 00026: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 258ms/step - loss: 0.0064 - acc: 0.9988 - val_loss: 0.2270 - val_acc: 0.9653\n",
            "Epoch 27/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0046 - acc: 0.9994\n",
            "Epoch 00027: val_loss did not improve from 0.14037\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "169/169 [==============================] - 44s 259ms/step - loss: 0.0046 - acc: 0.9994 - val_loss: 0.2277 - val_acc: 0.9636\n",
            "Epoch 28/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9988\n",
            "Epoch 00028: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.2280 - val_acc: 0.9644\n",
            "Epoch 29/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0055 - acc: 0.9988\n",
            "Epoch 00029: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 263ms/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.2281 - val_acc: 0.9644\n",
            "Epoch 30/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0041 - acc: 0.9994\n",
            "Epoch 00030: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 259ms/step - loss: 0.0041 - acc: 0.9994 - val_loss: 0.2285 - val_acc: 0.9644\n",
            "Epoch 31/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0056 - acc: 0.9985\n",
            "Epoch 00031: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0056 - acc: 0.9985 - val_loss: 0.2283 - val_acc: 0.9653\n",
            "Epoch 32/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0056 - acc: 0.9988\n",
            "Epoch 00032: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.2286 - val_acc: 0.9653\n",
            "Epoch 33/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0041 - acc: 0.9994\n",
            "Epoch 00033: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0041 - acc: 0.9994 - val_loss: 0.2290 - val_acc: 0.9653\n",
            "Epoch 34/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0040 - acc: 0.9994\n",
            "Epoch 00034: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 45s 264ms/step - loss: 0.0040 - acc: 0.9994 - val_loss: 0.2293 - val_acc: 0.9653\n",
            "Epoch 35/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0038 - acc: 0.9994\n",
            "Epoch 00035: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0038 - acc: 0.9994 - val_loss: 0.2297 - val_acc: 0.9653\n",
            "Epoch 36/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0058 - acc: 0.9982\n",
            "Epoch 00036: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 45s 264ms/step - loss: 0.0058 - acc: 0.9982 - val_loss: 0.2301 - val_acc: 0.9653\n",
            "Epoch 37/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0062 - acc: 0.9976\n",
            "Epoch 00037: val_loss did not improve from 0.14037\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0062 - acc: 0.9976 - val_loss: 0.2302 - val_acc: 0.9644\n",
            "Epoch 38/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0037 - acc: 0.9997\n",
            "Epoch 00038: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0037 - acc: 0.9997 - val_loss: 0.2302 - val_acc: 0.9653\n",
            "Epoch 39/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0035 - acc: 0.9991\n",
            "Epoch 00039: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.2302 - val_acc: 0.9653\n",
            "Epoch 40/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0059 - acc: 0.9976\n",
            "Epoch 00040: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0059 - acc: 0.9976 - val_loss: 0.2303 - val_acc: 0.9653\n",
            "Epoch 41/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0062 - acc: 0.9988\n",
            "Epoch 00041: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0062 - acc: 0.9988 - val_loss: 0.2303 - val_acc: 0.9653\n",
            "Epoch 42/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0042 - acc: 0.9994\n",
            "Epoch 00042: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0042 - acc: 0.9994 - val_loss: 0.2303 - val_acc: 0.9653\n",
            "Epoch 43/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0058 - acc: 0.9985\n",
            "Epoch 00043: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0058 - acc: 0.9985 - val_loss: 0.2303 - val_acc: 0.9653\n",
            "Epoch 44/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0047 - acc: 0.9985\n",
            "Epoch 00044: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0047 - acc: 0.9985 - val_loss: 0.2303 - val_acc: 0.9653\n",
            "Epoch 45/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0050 - acc: 0.9988\n",
            "Epoch 00045: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0050 - acc: 0.9988 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 46/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0063 - acc: 0.9979\n",
            "Epoch 00046: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 259ms/step - loss: 0.0063 - acc: 0.9979 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 47/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0058 - acc: 0.9985\n",
            "Epoch 00047: val_loss did not improve from 0.14037\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0058 - acc: 0.9985 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 48/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0052 - acc: 0.9979\n",
            "Epoch 00048: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0052 - acc: 0.9979 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 49/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
            "Epoch 00049: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0040 - acc: 0.9991 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 50/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0052 - acc: 0.9985\n",
            "Epoch 00050: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 51/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0047 - acc: 0.9988\n",
            "Epoch 00051: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 52/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0042 - acc: 0.9991\n",
            "Epoch 00052: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0042 - acc: 0.9991 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 53/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0042 - acc: 0.9991\n",
            "Epoch 00053: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0042 - acc: 0.9991 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 54/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0052 - acc: 0.9982\n",
            "Epoch 00054: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0052 - acc: 0.9982 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 55/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9979\n",
            "Epoch 00055: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0057 - acc: 0.9979 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 56/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0062 - acc: 0.9976\n",
            "Epoch 00056: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0062 - acc: 0.9976 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 57/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0041 - acc: 0.9991\n",
            "Epoch 00057: val_loss did not improve from 0.14037\n",
            "\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0041 - acc: 0.9991 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 58/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0048 - acc: 0.9994\n",
            "Epoch 00058: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0048 - acc: 0.9994 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 59/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0059 - acc: 0.9979\n",
            "Epoch 00059: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0059 - acc: 0.9979 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 60/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0033 - acc: 0.9988\n",
            "Epoch 00060: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0033 - acc: 0.9988 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 61/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0060 - acc: 0.9982\n",
            "Epoch 00061: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 62/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0075 - acc: 0.9979\n",
            "Epoch 00062: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0075 - acc: 0.9979 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 63/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0055 - acc: 0.9988\n",
            "Epoch 00063: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 64/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0046 - acc: 0.9985\n",
            "Epoch 00064: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0046 - acc: 0.9985 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 65/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0041 - acc: 0.9991\n",
            "Epoch 00065: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0041 - acc: 0.9991 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 66/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9982\n",
            "Epoch 00066: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0057 - acc: 0.9982 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 67/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0034 - acc: 0.9997\n",
            "Epoch 00067: val_loss did not improve from 0.14037\n",
            "\n",
            "Epoch 00067: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0034 - acc: 0.9997 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 68/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0053 - acc: 0.9982\n",
            "Epoch 00068: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 69/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0042 - acc: 0.9997\n",
            "Epoch 00069: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0042 - acc: 0.9997 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 70/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0032 - acc: 0.9991\n",
            "Epoch 00070: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0032 - acc: 0.9991 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 71/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0063 - acc: 0.9982\n",
            "Epoch 00071: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 72/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0039 - acc: 0.9994\n",
            "Epoch 00072: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0039 - acc: 0.9994 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 73/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0048 - acc: 0.9991\n",
            "Epoch 00073: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0048 - acc: 0.9991 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 74/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9979\n",
            "Epoch 00074: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 260ms/step - loss: 0.0057 - acc: 0.9979 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 75/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0044 - acc: 0.9988\n",
            "Epoch 00075: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 76/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0060 - acc: 0.9982\n",
            "Epoch 00076: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 77/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0046 - acc: 0.9985\n",
            "Epoch 00077: val_loss did not improve from 0.14037\n",
            "\n",
            "Epoch 00077: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0046 - acc: 0.9985 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 78/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0055 - acc: 0.9988\n",
            "Epoch 00078: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 79/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0048 - acc: 0.9994\n",
            "Epoch 00079: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0048 - acc: 0.9994 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 80/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0053 - acc: 0.9982\n",
            "Epoch 00080: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 81/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0034 - acc: 0.9994\n",
            "Epoch 00081: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0034 - acc: 0.9994 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 82/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0058 - acc: 0.9976\n",
            "Epoch 00082: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0058 - acc: 0.9976 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 83/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0040 - acc: 0.9988\n",
            "Epoch 00083: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 263ms/step - loss: 0.0040 - acc: 0.9988 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 84/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0052 - acc: 0.9979\n",
            "Epoch 00084: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0052 - acc: 0.9979 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 85/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0052 - acc: 0.9985\n",
            "Epoch 00085: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 86/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0062 - acc: 0.9982\n",
            "Epoch 00086: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 263ms/step - loss: 0.0062 - acc: 0.9982 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 87/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0069 - acc: 0.9976\n",
            "Epoch 00087: val_loss did not improve from 0.14037\n",
            "\n",
            "Epoch 00087: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0069 - acc: 0.9976 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 88/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0039 - acc: 0.9988\n",
            "Epoch 00088: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 89/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0059 - acc: 0.9988\n",
            "Epoch 00089: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0059 - acc: 0.9988 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 90/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0043 - acc: 0.9994\n",
            "Epoch 00090: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0043 - acc: 0.9994 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 91/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0050 - acc: 0.9991\n",
            "Epoch 00091: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0050 - acc: 0.9991 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 92/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0037 - acc: 0.9994\n",
            "Epoch 00092: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 263ms/step - loss: 0.0037 - acc: 0.9994 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 93/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0058 - acc: 0.9982\n",
            "Epoch 00093: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 45s 264ms/step - loss: 0.0058 - acc: 0.9982 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 94/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0038 - acc: 0.9994\n",
            "Epoch 00094: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0038 - acc: 0.9994 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 95/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0065 - acc: 0.9979\n",
            "Epoch 00095: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0065 - acc: 0.9979 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 96/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0056 - acc: 0.9982\n",
            "Epoch 00096: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0056 - acc: 0.9982 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 97/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0053 - acc: 0.9982\n",
            "Epoch 00097: val_loss did not improve from 0.14037\n",
            "\n",
            "Epoch 00097: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 98/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0059 - acc: 0.9979\n",
            "Epoch 00098: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0059 - acc: 0.9979 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 99/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0044 - acc: 0.9985\n",
            "Epoch 00099: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 262ms/step - loss: 0.0044 - acc: 0.9985 - val_loss: 0.2304 - val_acc: 0.9653\n",
            "Epoch 100/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0054 - acc: 0.9985\n",
            "Epoch 00100: val_loss did not improve from 0.14037\n",
            "169/169 [==============================] - 44s 261ms/step - loss: 0.0054 - acc: 0.9985 - val_loss: 0.2304 - val_acc: 0.9653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPJK06xF6tDj",
        "outputId": "3b26b707-0f0d-479d-ca77-d8c695eab73a"
      },
      "source": [
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.230394184589386\n",
            "Test accuracy: 0.9653333425521851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "2m67fTHc6tDj",
        "outputId": "603e9e97-0bb2-4abf-93cb-94812b603469"
      },
      "source": [
        "plt.plot(history.history['acc'],label=\"accuracy\")\n",
        "plt.plot(history.history['val_acc'],label=\"val_accuracy\")\n",
        "plt.plot(history.history['loss'],label=\"loss\")\n",
        "plt.plot(history.history['val_loss'],label=\"val_loss\")\n",
        "plt.legend()\n",
        "plt.title(\"FR Feature Selection\")\n",
        "plt.xlabel('epoch')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'epoch')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqBcJ8xX6tDj"
      },
      "source": [
        "## Save the model/weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9EkryyD6tDj",
        "outputId": "f4df88d5-5ee1-44ce-a678-aa1047c21705"
      },
      "source": [
        "# JSON JSON\n",
        "# serialize model to json\n",
        "json_model = model.to_json()\n",
        "\n",
        "# save the model architecture to JSON file\n",
        "with open('{}/{}.model.json'.format(output_dir, model_name), 'w') as json_file:\n",
        "    json_file.write(json_model)\n",
        "\n",
        "\n",
        "# YAML YAML\n",
        "# serialize model to YAML\n",
        "model_yaml = model.to_yaml()\n",
        "\n",
        "# save the model architecture to YAML file\n",
        "with open(\"{}/{}.model.yaml\".format(output_dir, model_name), \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "\n",
        "\n",
        "# WEIGHTS HDF5\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"{}/{}.model.h5\".format(output_dir,model_name))\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnn8quy36tDk",
        "outputId": "fe75d8a5-c618-4053-c9f9-d9f3ca0736a9"
      },
      "source": [
        "# Open the handle\n",
        "json_file = open('{}/{}.model.json'.format(output_dir, model_name), 'r')\n",
        "\n",
        "# load json and create model\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model.load_weights('{}/{}.model.h5'.format(output_dir, model_name))\n",
        "print(\"Loaded model from disk\")\n",
        "# loaded_model_json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOH5Uzwz6tDl",
        "outputId": "311c977c-3917-4b7b-e3d3-53611146ab21"
      },
      "source": [
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='categorical_crossentropy', optimizer='sgd', \n",
        "                     metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.230394184589386\n",
            "Test accuracy: 0.9653333425521851\n",
            "accuracy: 96.53%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfNdknXdWMIF"
      },
      "source": [
        "# ConvNN(RF) - top 300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUXRYNVOWMIJ"
      },
      "source": [
        "## Train/Test split    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KZUSHxfWMIK"
      },
      "source": [
        "# from keras.utils import to_categorical\n",
        "# outcome = encode(outcome['Project_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRyPQL84WMIK",
        "outputId": "b8331172-e5fa-4a27-cce2-d1bf7b6daef1"
      },
      "source": [
        "feature_sort_selected = feature_sort.iloc[:300,:]\n",
        "feature_sort_selected"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10258</th>\n",
              "      <td>SLC45A3</td>\n",
              "      <td>0.003607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36533</th>\n",
              "      <td>CTD-2182N23.1</td>\n",
              "      <td>0.003576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29180</th>\n",
              "      <td>AC012123.1</td>\n",
              "      <td>0.003306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6445</th>\n",
              "      <td>NAPSA</td>\n",
              "      <td>0.002897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12521</th>\n",
              "      <td>C8orf46</td>\n",
              "      <td>0.002802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3724</th>\n",
              "      <td>B3GAT1</td>\n",
              "      <td>0.000643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29500</th>\n",
              "      <td>ORM1</td>\n",
              "      <td>0.000643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46193</th>\n",
              "      <td>RP11-320M16.1</td>\n",
              "      <td>0.000643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37443</th>\n",
              "      <td>CTD-2377D24.4</td>\n",
              "      <td>0.000640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4173</th>\n",
              "      <td>CDH6</td>\n",
              "      <td>0.000640</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>300 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                name  importance\n",
              "10258        SLC45A3    0.003607\n",
              "36533  CTD-2182N23.1    0.003576\n",
              "29180     AC012123.1    0.003306\n",
              "6445           NAPSA    0.002897\n",
              "12521        C8orf46    0.002802\n",
              "...              ...         ...\n",
              "3724          B3GAT1    0.000643\n",
              "29500           ORM1    0.000643\n",
              "46193  RP11-320M16.1    0.000643\n",
              "37443  CTD-2377D24.4    0.000640\n",
              "4173            CDH6    0.000640\n",
              "\n",
              "[300 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKPm44RhWMIL"
      },
      "source": [
        "TC1data15_selected = TC1data15.loc[:,feature_sort_selected['name']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6p7sF87WMIN"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(TC1data15_selected, \n",
        "                                                    outcome, \n",
        "                                                    train_size=0.75, \n",
        "                                                    test_size=0.25, \n",
        "                                                    random_state=123, \n",
        "                                                    stratify = outcome)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cztMXzlVWMIN",
        "outputId": "7bf5a14f-27a8-441f-e7d3-4311e5555ff5"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SLC45A3</th>\n",
              "      <th>CTD-2182N23.1</th>\n",
              "      <th>AC012123.1</th>\n",
              "      <th>NAPSA</th>\n",
              "      <th>C8orf46</th>\n",
              "      <th>AL161626.1</th>\n",
              "      <th>RP11-264B14.2</th>\n",
              "      <th>FP236383.9</th>\n",
              "      <th>SOX17</th>\n",
              "      <th>ACSM2B</th>\n",
              "      <th>PAX8</th>\n",
              "      <th>U47924.27</th>\n",
              "      <th>PSD2</th>\n",
              "      <th>FP671120.6</th>\n",
              "      <th>TRPS1</th>\n",
              "      <th>NKX2-1-AS1</th>\n",
              "      <th>CHRNA2</th>\n",
              "      <th>TSHR</th>\n",
              "      <th>GAL3ST1</th>\n",
              "      <th>SLC28A1</th>\n",
              "      <th>APOC3</th>\n",
              "      <th>Y_RNA.262</th>\n",
              "      <th>C8B</th>\n",
              "      <th>SNRPGP10</th>\n",
              "      <th>RP11-25I15.1</th>\n",
              "      <th>SLC17A3</th>\n",
              "      <th>SLC2A2</th>\n",
              "      <th>SLC39A5</th>\n",
              "      <th>TM4SF5</th>\n",
              "      <th>PCA3</th>\n",
              "      <th>RMST</th>\n",
              "      <th>ATP8A2P1</th>\n",
              "      <th>MGAT4C</th>\n",
              "      <th>HEPH</th>\n",
              "      <th>ORM2</th>\n",
              "      <th>PLG</th>\n",
              "      <th>CRYGN</th>\n",
              "      <th>RP11-53M11.5</th>\n",
              "      <th>TRABD2A</th>\n",
              "      <th>IYD</th>\n",
              "      <th>...</th>\n",
              "      <th>GPR157</th>\n",
              "      <th>SCTR</th>\n",
              "      <th>PPAP2B</th>\n",
              "      <th>LCN12</th>\n",
              "      <th>MEIS1</th>\n",
              "      <th>TMEM243</th>\n",
              "      <th>LLNLF-187D8.1</th>\n",
              "      <th>F9</th>\n",
              "      <th>NCAN</th>\n",
              "      <th>AC021218.2</th>\n",
              "      <th>DOCK3</th>\n",
              "      <th>CAPN13</th>\n",
              "      <th>TPO</th>\n",
              "      <th>LINC00320</th>\n",
              "      <th>PHKG1</th>\n",
              "      <th>LRRN1</th>\n",
              "      <th>DNER</th>\n",
              "      <th>GRIA3</th>\n",
              "      <th>S100A2</th>\n",
              "      <th>NPY</th>\n",
              "      <th>FP671120.5</th>\n",
              "      <th>FAM83H</th>\n",
              "      <th>APOB</th>\n",
              "      <th>RP11-2E11.5</th>\n",
              "      <th>MASP2</th>\n",
              "      <th>PGC</th>\n",
              "      <th>AC012354.8</th>\n",
              "      <th>CPB2</th>\n",
              "      <th>GRHL2</th>\n",
              "      <th>NAAA</th>\n",
              "      <th>KLHL35</th>\n",
              "      <th>KRT6A</th>\n",
              "      <th>KCNJ10</th>\n",
              "      <th>SERPINB13</th>\n",
              "      <th>RP11-429B14.4</th>\n",
              "      <th>B3GAT1</th>\n",
              "      <th>ORM1</th>\n",
              "      <th>RP11-320M16.1</th>\n",
              "      <th>CTD-2377D24.4</th>\n",
              "      <th>CDH6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>849</th>\n",
              "      <td>0.717435</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.672119</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.424186</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.170832</td>\n",
              "      <td>1.493830</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.094042</td>\n",
              "      <td>1.026745</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.924745</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079015</td>\n",
              "      <td>0.615134</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.229850</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.752871</td>\n",
              "      <td>1.765075</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.184754</td>\n",
              "      <td>0.529635</td>\n",
              "      <td>...</td>\n",
              "      <td>1.065158</td>\n",
              "      <td>0.733559</td>\n",
              "      <td>1.580060</td>\n",
              "      <td>0.556286</td>\n",
              "      <td>0.571990</td>\n",
              "      <td>1.290002</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.357876</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.028038</td>\n",
              "      <td>0.700337</td>\n",
              "      <td>0.343320</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.018920</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2.045059</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.095648</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.794982</td>\n",
              "      <td>1.636991</td>\n",
              "      <td>1.061681</td>\n",
              "      <td>0.603725</td>\n",
              "      <td>1.469228</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.223209</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.272571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1050</th>\n",
              "      <td>0.901859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.732119</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.257574</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.420687</td>\n",
              "      <td>0.861244</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.566138</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.137554</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.473346</td>\n",
              "      <td>0.452157</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.780366</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.993561</td>\n",
              "      <td>2.947773</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.252547</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.716243</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.473386</td>\n",
              "      <td>1.023498</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.121711</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.624630</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2.433008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.734314</td>\n",
              "      <td>1.367369</td>\n",
              "      <td>0.239177</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.375890</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>1.169069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.690877</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365155</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041478</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.848804</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.157242</td>\n",
              "      <td>0.557529</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.914976</td>\n",
              "      <td>0.622720</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.659029</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.988589</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.319069</td>\n",
              "      <td>0.744653</td>\n",
              "      <td>0.793214</td>\n",
              "      <td>1.069496</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.594582</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.134651</td>\n",
              "      <td>0.553795</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.626405</td>\n",
              "      <td>1.165371</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2.104592</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.713397</td>\n",
              "      <td>0.566962</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.511951</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.746622</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4447</th>\n",
              "      <td>1.001501</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.393282</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.284720</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.510884</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039429</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.537337</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.589360</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.339144</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.411680</td>\n",
              "      <td>0.079273</td>\n",
              "      <td>0.714604</td>\n",
              "      <td>1.132097</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.056761</td>\n",
              "      <td>0.780352</td>\n",
              "      <td>0.730571</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.035213</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.613861</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.809261</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.153790</td>\n",
              "      <td>0.885525</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.281632</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.151640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0.429030</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.698742</td>\n",
              "      <td>0.094371</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.670652</td>\n",
              "      <td>0.708519</td>\n",
              "      <td>2.400472</td>\n",
              "      <td>2.316836</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.962383</td>\n",
              "      <td>1.967081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.948668</td>\n",
              "      <td>1.550796</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365934</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.092616</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.604855</td>\n",
              "      <td>1.268017</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.480250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.133977</td>\n",
              "      <td>0.475185</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.337691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.640065</td>\n",
              "      <td>1.325293</td>\n",
              "      <td>1.494002</td>\n",
              "      <td>1.258171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.766147</td>\n",
              "      <td>0.963744</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.091356</td>\n",
              "      <td>1.227083</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.804598</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.37167</td>\n",
              "      <td>2.186938</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.721946</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.543859</td>\n",
              "      <td>0.980574</td>\n",
              "      <td>0.875821</td>\n",
              "      <td>0.100771</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.035793</td>\n",
              "      <td>0.581620</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.625373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3495</th>\n",
              "      <td>1.558939</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.636459</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.052139</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.436981</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.659693</td>\n",
              "      <td>0.337474</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.196452</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.168646</td>\n",
              "      <td>...</td>\n",
              "      <td>1.432803</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.592233</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.107061</td>\n",
              "      <td>1.099255</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.086653</td>\n",
              "      <td>1.360833</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.570652</td>\n",
              "      <td>0.513463</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.968258</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2.103458</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.585684</td>\n",
              "      <td>0.873183</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.753029</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.144879</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.545132</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.320591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1737</th>\n",
              "      <td>0.508131</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.234141</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.229859</td>\n",
              "      <td>1.537464</td>\n",
              "      <td>1.877514</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.735723</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.093082</td>\n",
              "      <td>1.849123</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.315884</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.722</td>\n",
              "      <td>1.62879</td>\n",
              "      <td>1.310074</td>\n",
              "      <td>1.645405</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.199691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.155142</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.730860</td>\n",
              "      <td>0.171624</td>\n",
              "      <td>2.428515</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.555550</td>\n",
              "      <td>1.450051</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.287644</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.126673</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.985606</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.408404</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.076519</td>\n",
              "      <td>0.298232</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.827543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2943</th>\n",
              "      <td>0.794707</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.148528</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.383600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.406436</td>\n",
              "      <td>1.454994</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.320577</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.268603</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.417348</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.399102</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.180971</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.814278</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.308324</td>\n",
              "      <td>1.101993</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.822678</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.949028</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.452902</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.661767</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.026789</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.582818</td>\n",
              "      <td>1.234726</td>\n",
              "      <td>0.475642</td>\n",
              "      <td>3.376670</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.432647</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.135977</td>\n",
              "      <td>0.067120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1573</th>\n",
              "      <td>0.891176</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.707764</td>\n",
              "      <td>0.152280</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.282959</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.620480</td>\n",
              "      <td>1.605914</td>\n",
              "      <td>0.051819</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.200227</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.238608</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.973891</td>\n",
              "      <td>0.225703</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.158866</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.991760</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.047212</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.101991</td>\n",
              "      <td>0.959958</td>\n",
              "      <td>1.826872</td>\n",
              "      <td>0.858018</td>\n",
              "      <td>0.018845</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.706036</td>\n",
              "      <td>0.966210</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.400407</td>\n",
              "      <td>1.194524</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.244948</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2.299847</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.787311</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.905957</td>\n",
              "      <td>0.339594</td>\n",
              "      <td>0.476849</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.219231</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.479118</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.279128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3192</th>\n",
              "      <td>0.440866</td>\n",
              "      <td>3.215473</td>\n",
              "      <td>0.649271</td>\n",
              "      <td>0.455298</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.852791</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.665235</td>\n",
              "      <td>1.730446</td>\n",
              "      <td>0.292689</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.725373</td>\n",
              "      <td>1.328306</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.821487</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.222551</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.055060</td>\n",
              "      <td>0.227691</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.75023</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.258463</td>\n",
              "      <td>0.762914</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.617423</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.566856</td>\n",
              "      <td>1.581944</td>\n",
              "      <td>...</td>\n",
              "      <td>0.592682</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.028560</td>\n",
              "      <td>2.253740</td>\n",
              "      <td>0.037470</td>\n",
              "      <td>1.966158</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.606123</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.574433</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.302902</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.540553</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.858464</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.065058</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.392420</td>\n",
              "      <td>0.962125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.482606</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.202549</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3375 rows × 300 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       SLC45A3  CTD-2182N23.1  ...  CTD-2377D24.4      CDH6\n",
              "849   0.717435       0.000000  ...       0.000000  0.272571\n",
              "1050  0.901859       0.000000  ...       0.000000  0.000000\n",
              "835   1.169069       0.000000  ...       0.000000  0.000000\n",
              "4447  1.001501       0.000000  ...       0.000000  0.151640\n",
              "53    0.429030       0.000000  ...       0.000000  1.625373\n",
              "...        ...            ...  ...            ...       ...\n",
              "3495  1.558939       0.000000  ...       0.000000  0.320591\n",
              "1737  0.508131       0.000000  ...       0.000000  1.827543\n",
              "2943  0.794707       0.000000  ...       1.135977  0.067120\n",
              "1573  0.891176       0.000000  ...       0.000000  2.279128\n",
              "3192  0.440866       3.215473  ...       0.000000  1.202549\n",
              "\n",
              "[3375 rows x 300 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smpF7T_vWMIN"
      },
      "source": [
        "## CONV1D "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-o8INSiWMIO"
      },
      "source": [
        "# parameters  \n",
        "activation='relu'\n",
        "batch_size=20\n",
        "# Number of sites\n",
        "classes=15\n",
        "drop = 0.1\n",
        "feature_subsample = 0\n",
        "loss='categorical_crossentropy'\n",
        "# metrics='accuracy'\n",
        "out_act='softmax'\n",
        "pool=[1, 10]\n",
        "# optimizer='sgd'\n",
        "shuffle = False \n",
        "epochs=100\n",
        "\n",
        "optimizer = optimizers.SGD(lr=0.1)\n",
        "metrics = ['acc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJXEj0FvWMIO"
      },
      "source": [
        "x_train_len = X_train.shape[1]   \n",
        "\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "filters = 128 \n",
        "filter_len = 20 \n",
        "stride = 1 \n",
        "\n",
        "# inside pool_list loop\n",
        "pool_list = [1,10]\n",
        "\n",
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvObEwDSWMIO",
        "outputId": "1dbca77f-c3dd-408e-c2d8-cb72ef026bdd"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# model.add  CONV1D\n",
        "model.add(Conv1D(filters = filters, \n",
        "                 kernel_size = filter_len, \n",
        "                 strides = stride, \n",
        "                 padding='valid', \n",
        "                 input_shape=(x_train_len, 1)))\n",
        "\n",
        "# Activation\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# MaxPooling\n",
        "model.add(MaxPooling1D(pool_size = 1))\n",
        "\n",
        "filters = 128\n",
        "filter_len = 10 \n",
        "stride = 1 \n",
        "# Conv1D\n",
        "model.add(Conv1D(filters=filters, \n",
        "                 kernel_size=filter_len, \n",
        "                 strides=stride, \n",
        "                 padding='valid'))\n",
        "# Activation\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# MaxPooling\n",
        "model.add(MaxPooling1D(pool_size = 10))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(200))\n",
        "\n",
        "# activation \n",
        "# model.add(Activation('relu')) # SR\n",
        "model.add(Activation(activation))\n",
        "#dropout\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(20))\n",
        "# activation\n",
        "# model.add(Activation('relu')) # SR\n",
        "model.add(Activation(activation))\n",
        "\n",
        "#dropout\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(15))\n",
        "model.add(Activation(out_act))\n",
        "\n",
        "model.compile( loss= loss, \n",
        "              optimizer = optimizer, \n",
        "              metrics = metrics )\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 281, 128)          2688      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 281, 128)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 281, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 272, 128)          163968    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 272, 128)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 27, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3456)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               691400    \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                4020      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 15)                315       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 862,391\n",
            "Trainable params: 862,391\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgho0oXYWMIP"
      },
      "source": [
        "# save\n",
        "# save = '/content/drive/My Drive/FNL_TC1/'\n",
        "output_dir = \"/content/drive/My Drive/FNL_TC1/Model\"\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "model_name = 'tc1_rf'\n",
        "path = '{}/{}.autosave.model.h5'.format(output_dir, model_name)\n",
        "checkpointer = ModelCheckpoint(filepath=path,\n",
        "                               verbose=1,\n",
        "                               save_weights_only=True,\n",
        "                               save_best_only=True)\n",
        "\n",
        "csv_logger = CSVLogger('{}/training.log'.format(output_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VG5goixWMIP"
      },
      "source": [
        "# SR: change epsilon to min_delta\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
        "                              factor=0.1, \n",
        "                              patience=10, \n",
        "                              verbose=1, mode='auto', \n",
        "                              min_delta=0.0001, \n",
        "                              cooldown=0, \n",
        "                              min_lr=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqSPynzGWMIP",
        "outputId": "fc130a1e-1cf1-4f27-ba00-9d596149fbbc"
      },
      "source": [
        "# batch_size = 20 \n",
        "history = model.fit(X_train, Y_train, batch_size=batch_size, \n",
        "                    epochs=epochs, verbose=1, validation_data=(X_test, Y_test), \n",
        "                    callbacks = [checkpointer, csv_logger, reduce_lr])\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 1.0476 - acc: 0.6643\n",
            "Epoch 00001: val_loss improved from inf to 0.29168, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 27s 158ms/step - loss: 1.0476 - acc: 0.6643 - val_loss: 0.2917 - val_acc: 0.9129\n",
            "Epoch 2/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.2865 - acc: 0.9102\n",
            "Epoch 00002: val_loss improved from 0.29168 to 0.24426, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 27s 158ms/step - loss: 0.2865 - acc: 0.9102 - val_loss: 0.2443 - val_acc: 0.9280\n",
            "Epoch 3/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1995 - acc: 0.9425\n",
            "Epoch 00003: val_loss did not improve from 0.24426\n",
            "169/169 [==============================] - 27s 158ms/step - loss: 0.1995 - acc: 0.9425 - val_loss: 0.2903 - val_acc: 0.9067\n",
            "Epoch 4/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1712 - acc: 0.9505\n",
            "Epoch 00004: val_loss improved from 0.24426 to 0.21828, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 27s 160ms/step - loss: 0.1712 - acc: 0.9505 - val_loss: 0.2183 - val_acc: 0.9342\n",
            "Epoch 5/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1288 - acc: 0.9579\n",
            "Epoch 00005: val_loss improved from 0.21828 to 0.18507, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 27s 160ms/step - loss: 0.1288 - acc: 0.9579 - val_loss: 0.1851 - val_acc: 0.9502\n",
            "Epoch 6/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.1071 - acc: 0.9644\n",
            "Epoch 00006: val_loss improved from 0.18507 to 0.18191, saving model to /content/drive/My Drive/FNL_TC1/Model/tc1_rf.autosave.model.h5\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.1071 - acc: 0.9644 - val_loss: 0.1819 - val_acc: 0.9547\n",
            "Epoch 7/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0999 - acc: 0.9710\n",
            "Epoch 00007: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 158ms/step - loss: 0.0999 - acc: 0.9710 - val_loss: 0.4008 - val_acc: 0.8809\n",
            "Epoch 8/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0895 - acc: 0.9745\n",
            "Epoch 00008: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 158ms/step - loss: 0.0895 - acc: 0.9745 - val_loss: 0.1951 - val_acc: 0.9422\n",
            "Epoch 9/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0731 - acc: 0.9766\n",
            "Epoch 00009: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 158ms/step - loss: 0.0731 - acc: 0.9766 - val_loss: 0.1993 - val_acc: 0.9458\n",
            "Epoch 10/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0657 - acc: 0.9775\n",
            "Epoch 00010: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 157ms/step - loss: 0.0657 - acc: 0.9775 - val_loss: 0.1986 - val_acc: 0.9556\n",
            "Epoch 11/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0629 - acc: 0.9807\n",
            "Epoch 00011: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 159ms/step - loss: 0.0629 - acc: 0.9807 - val_loss: 0.2029 - val_acc: 0.9520\n",
            "Epoch 12/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0497 - acc: 0.9819\n",
            "Epoch 00012: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0497 - acc: 0.9819 - val_loss: 0.1861 - val_acc: 0.9609\n",
            "Epoch 13/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0490 - acc: 0.9852\n",
            "Epoch 00013: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0490 - acc: 0.9852 - val_loss: 0.1919 - val_acc: 0.9493\n",
            "Epoch 14/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0363 - acc: 0.9864\n",
            "Epoch 00014: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0363 - acc: 0.9864 - val_loss: 0.2164 - val_acc: 0.9484\n",
            "Epoch 15/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0473 - acc: 0.9849\n",
            "Epoch 00015: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0473 - acc: 0.9849 - val_loss: 0.1827 - val_acc: 0.9458\n",
            "Epoch 16/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0248 - acc: 0.9926\n",
            "Epoch 00016: val_loss did not improve from 0.18191\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0248 - acc: 0.9926 - val_loss: 0.1928 - val_acc: 0.9600\n",
            "Epoch 17/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0126 - acc: 0.9961\n",
            "Epoch 00017: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0126 - acc: 0.9961 - val_loss: 0.1989 - val_acc: 0.9591\n",
            "Epoch 18/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0119 - acc: 0.9961\n",
            "Epoch 00018: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 160ms/step - loss: 0.0119 - acc: 0.9961 - val_loss: 0.2021 - val_acc: 0.9591\n",
            "Epoch 19/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0084 - acc: 0.9979\n",
            "Epoch 00019: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 160ms/step - loss: 0.0084 - acc: 0.9979 - val_loss: 0.2065 - val_acc: 0.9591\n",
            "Epoch 20/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0111 - acc: 0.9967\n",
            "Epoch 00020: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0111 - acc: 0.9967 - val_loss: 0.2102 - val_acc: 0.9591\n",
            "Epoch 21/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0092 - acc: 0.9967\n",
            "Epoch 00021: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 160ms/step - loss: 0.0092 - acc: 0.9967 - val_loss: 0.2102 - val_acc: 0.9609\n",
            "Epoch 22/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0091 - acc: 0.9973\n",
            "Epoch 00022: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 160ms/step - loss: 0.0091 - acc: 0.9973 - val_loss: 0.2124 - val_acc: 0.9582\n",
            "Epoch 23/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0099 - acc: 0.9970\n",
            "Epoch 00023: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.2161 - val_acc: 0.9573\n",
            "Epoch 24/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0090 - acc: 0.9970\n",
            "Epoch 00024: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0090 - acc: 0.9970 - val_loss: 0.2142 - val_acc: 0.9618\n",
            "Epoch 25/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0080 - acc: 0.9982\n",
            "Epoch 00025: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0080 - acc: 0.9982 - val_loss: 0.2159 - val_acc: 0.9600\n",
            "Epoch 26/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0052 - acc: 0.9991\n",
            "Epoch 00026: val_loss did not improve from 0.18191\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0052 - acc: 0.9991 - val_loss: 0.2186 - val_acc: 0.9582\n",
            "Epoch 27/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9988\n",
            "Epoch 00027: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.2188 - val_acc: 0.9600\n",
            "Epoch 28/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0050 - acc: 0.9997\n",
            "Epoch 00028: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0050 - acc: 0.9997 - val_loss: 0.2192 - val_acc: 0.9600\n",
            "Epoch 29/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0047 - acc: 0.9988\n",
            "Epoch 00029: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.2195 - val_acc: 0.9600\n",
            "Epoch 30/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0070 - acc: 0.9979\n",
            "Epoch 00030: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 165ms/step - loss: 0.0070 - acc: 0.9979 - val_loss: 0.2194 - val_acc: 0.9600\n",
            "Epoch 31/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0104 - acc: 0.9970\n",
            "Epoch 00031: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.2194 - val_acc: 0.9600\n",
            "Epoch 32/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0065 - acc: 0.9982\n",
            "Epoch 00032: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 164ms/step - loss: 0.0065 - acc: 0.9982 - val_loss: 0.2194 - val_acc: 0.9600\n",
            "Epoch 33/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0075 - acc: 0.9982\n",
            "Epoch 00033: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 166ms/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.2194 - val_acc: 0.9600\n",
            "Epoch 34/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0070 - acc: 0.9988\n",
            "Epoch 00034: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0070 - acc: 0.9988 - val_loss: 0.2200 - val_acc: 0.9600\n",
            "Epoch 35/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9976\n",
            "Epoch 00035: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.2201 - val_acc: 0.9600\n",
            "Epoch 36/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0072 - acc: 0.9988\n",
            "Epoch 00036: val_loss did not improve from 0.18191\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0072 - acc: 0.9988 - val_loss: 0.2202 - val_acc: 0.9600\n",
            "Epoch 37/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0090 - acc: 0.9967\n",
            "Epoch 00037: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 160ms/step - loss: 0.0090 - acc: 0.9967 - val_loss: 0.2202 - val_acc: 0.9600\n",
            "Epoch 38/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0063 - acc: 0.9988\n",
            "Epoch 00038: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 160ms/step - loss: 0.0063 - acc: 0.9988 - val_loss: 0.2202 - val_acc: 0.9600\n",
            "Epoch 39/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0056 - acc: 0.9991\n",
            "Epoch 00039: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0056 - acc: 0.9991 - val_loss: 0.2203 - val_acc: 0.9600\n",
            "Epoch 40/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0067 - acc: 0.9985\n",
            "Epoch 00040: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0067 - acc: 0.9985 - val_loss: 0.2203 - val_acc: 0.9600\n",
            "Epoch 41/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0091 - acc: 0.9979\n",
            "Epoch 00041: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0091 - acc: 0.9979 - val_loss: 0.2203 - val_acc: 0.9600\n",
            "Epoch 42/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0058 - acc: 0.9988\n",
            "Epoch 00042: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 163ms/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.2203 - val_acc: 0.9600\n",
            "Epoch 43/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9982\n",
            "Epoch 00043: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0057 - acc: 0.9982 - val_loss: 0.2203 - val_acc: 0.9600\n",
            "Epoch 44/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0063 - acc: 0.9982\n",
            "Epoch 00044: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.2203 - val_acc: 0.9600\n",
            "Epoch 45/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0083 - acc: 0.9973\n",
            "Epoch 00045: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0083 - acc: 0.9973 - val_loss: 0.2203 - val_acc: 0.9600\n",
            "Epoch 46/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0082 - acc: 0.9991\n",
            "Epoch 00046: val_loss did not improve from 0.18191\n",
            "\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0082 - acc: 0.9991 - val_loss: 0.2203 - val_acc: 0.9600\n",
            "Epoch 47/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0058 - acc: 0.9991\n",
            "Epoch 00047: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0058 - acc: 0.9991 - val_loss: 0.2203 - val_acc: 0.9600\n",
            "Epoch 48/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0062 - acc: 0.9988\n",
            "Epoch 00048: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0062 - acc: 0.9988 - val_loss: 0.2203 - val_acc: 0.9600\n",
            "Epoch 49/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0071 - acc: 0.9988\n",
            "Epoch 00049: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0071 - acc: 0.9988 - val_loss: 0.2203 - val_acc: 0.9600\n",
            "Epoch 50/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0059 - acc: 0.9994\n",
            "Epoch 00050: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 163ms/step - loss: 0.0059 - acc: 0.9994 - val_loss: 0.2203 - val_acc: 0.9600\n",
            "Epoch 51/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0065 - acc: 0.9988\n",
            "Epoch 00051: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0065 - acc: 0.9988 - val_loss: 0.2203 - val_acc: 0.9600\n",
            "Epoch 52/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0058 - acc: 0.9985\n",
            "Epoch 00052: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0058 - acc: 0.9985 - val_loss: 0.2203 - val_acc: 0.9600\n",
            "Epoch 53/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0059 - acc: 0.9985\n",
            "Epoch 00053: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0059 - acc: 0.9985 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 54/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0061 - acc: 0.9991\n",
            "Epoch 00054: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0061 - acc: 0.9991 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 55/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0107 - acc: 0.9967\n",
            "Epoch 00055: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0107 - acc: 0.9967 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 56/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0067 - acc: 0.9988\n",
            "Epoch 00056: val_loss did not improve from 0.18191\n",
            "\n",
            "Epoch 00056: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
            "169/169 [==============================] - 27s 163ms/step - loss: 0.0067 - acc: 0.9988 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 57/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0059 - acc: 0.9991\n",
            "Epoch 00057: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0059 - acc: 0.9991 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 58/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0052 - acc: 0.9988\n",
            "Epoch 00058: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0052 - acc: 0.9988 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 59/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0062 - acc: 0.9985\n",
            "Epoch 00059: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 164ms/step - loss: 0.0062 - acc: 0.9985 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 60/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0066 - acc: 0.9982\n",
            "Epoch 00060: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0066 - acc: 0.9982 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 61/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0066 - acc: 0.9979\n",
            "Epoch 00061: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0066 - acc: 0.9979 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 62/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0077 - acc: 0.9982\n",
            "Epoch 00062: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 164ms/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 63/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0067 - acc: 0.9982\n",
            "Epoch 00063: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0067 - acc: 0.9982 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 64/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0079 - acc: 0.9982\n",
            "Epoch 00064: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 161ms/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 65/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0065 - acc: 0.9988\n",
            "Epoch 00065: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0065 - acc: 0.9988 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 66/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0060 - acc: 0.9988\n",
            "Epoch 00066: val_loss did not improve from 0.18191\n",
            "\n",
            "Epoch 00066: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
            "169/169 [==============================] - 28s 164ms/step - loss: 0.0060 - acc: 0.9988 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 67/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0053 - acc: 0.9991\n",
            "Epoch 00067: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 166ms/step - loss: 0.0053 - acc: 0.9991 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 68/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0052 - acc: 0.9991\n",
            "Epoch 00068: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0052 - acc: 0.9991 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 69/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0067 - acc: 0.9988\n",
            "Epoch 00069: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0067 - acc: 0.9988 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 70/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0055 - acc: 0.9988\n",
            "Epoch 00070: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 71/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0078 - acc: 0.9976\n",
            "Epoch 00071: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 72/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0065 - acc: 0.9985\n",
            "Epoch 00072: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 163ms/step - loss: 0.0065 - acc: 0.9985 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 73/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0056 - acc: 0.9988\n",
            "Epoch 00073: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 164ms/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 74/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0068 - acc: 0.9970\n",
            "Epoch 00074: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0068 - acc: 0.9970 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 75/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0054 - acc: 0.9991\n",
            "Epoch 00075: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 163ms/step - loss: 0.0054 - acc: 0.9991 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 76/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0091 - acc: 0.9979\n",
            "Epoch 00076: val_loss did not improve from 0.18191\n",
            "\n",
            "Epoch 00076: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0091 - acc: 0.9979 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 77/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0058 - acc: 0.9985\n",
            "Epoch 00077: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 165ms/step - loss: 0.0058 - acc: 0.9985 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 78/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0073 - acc: 0.9985\n",
            "Epoch 00078: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 165ms/step - loss: 0.0073 - acc: 0.9985 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 79/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0073 - acc: 0.9979\n",
            "Epoch 00079: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0073 - acc: 0.9979 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 80/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0072 - acc: 0.9982\n",
            "Epoch 00080: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 164ms/step - loss: 0.0072 - acc: 0.9982 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 81/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0069 - acc: 0.9976\n",
            "Epoch 00081: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 168ms/step - loss: 0.0069 - acc: 0.9976 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 82/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9988\n",
            "Epoch 00082: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 166ms/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 83/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0066 - acc: 0.9982\n",
            "Epoch 00083: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 164ms/step - loss: 0.0066 - acc: 0.9982 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 84/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0079 - acc: 0.9985\n",
            "Epoch 00084: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 166ms/step - loss: 0.0079 - acc: 0.9985 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 85/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9985\n",
            "Epoch 00085: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 166ms/step - loss: 0.0057 - acc: 0.9985 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 86/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0072 - acc: 0.9982\n",
            "Epoch 00086: val_loss did not improve from 0.18191\n",
            "\n",
            "Epoch 00086: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
            "169/169 [==============================] - 28s 164ms/step - loss: 0.0072 - acc: 0.9982 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 87/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0065 - acc: 0.9985\n",
            "Epoch 00087: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0065 - acc: 0.9985 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 88/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0075 - acc: 0.9979\n",
            "Epoch 00088: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0075 - acc: 0.9979 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 89/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0063 - acc: 0.9985\n",
            "Epoch 00089: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 165ms/step - loss: 0.0063 - acc: 0.9985 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 90/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0046 - acc: 0.9994\n",
            "Epoch 00090: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0046 - acc: 0.9994 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 91/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0045 - acc: 0.9988\n",
            "Epoch 00091: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 165ms/step - loss: 0.0045 - acc: 0.9988 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 92/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0049 - acc: 0.9988\n",
            "Epoch 00092: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0049 - acc: 0.9988 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 93/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0047 - acc: 0.9997\n",
            "Epoch 00093: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0047 - acc: 0.9997 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 94/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0071 - acc: 0.9979\n",
            "Epoch 00094: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0071 - acc: 0.9979 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 95/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0049 - acc: 0.9997\n",
            "Epoch 00095: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 28s 163ms/step - loss: 0.0049 - acc: 0.9997 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 96/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0047 - acc: 0.9997\n",
            "Epoch 00096: val_loss did not improve from 0.18191\n",
            "\n",
            "Epoch 00096: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\n",
            "169/169 [==============================] - 28s 164ms/step - loss: 0.0047 - acc: 0.9997 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 97/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0065 - acc: 0.9985\n",
            "Epoch 00097: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0065 - acc: 0.9985 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 98/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0067 - acc: 0.9985\n",
            "Epoch 00098: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0067 - acc: 0.9985 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 99/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0074 - acc: 0.9976\n",
            "Epoch 00099: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.2204 - val_acc: 0.9600\n",
            "Epoch 100/100\n",
            "169/169 [==============================] - ETA: 0s - loss: 0.0063 - acc: 0.9985\n",
            "Epoch 00100: val_loss did not improve from 0.18191\n",
            "169/169 [==============================] - 27s 162ms/step - loss: 0.0063 - acc: 0.9985 - val_loss: 0.2204 - val_acc: 0.9600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGgmmmbNWMIP",
        "outputId": "bca3790b-179f-4d3b-af92-081e5799a743"
      },
      "source": [
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.22035759687423706\n",
            "Test accuracy: 0.9599999785423279\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "krBeAg-2WMIQ",
        "outputId": "a2c58440-180c-47c1-bf02-886db6d97641"
      },
      "source": [
        "plt.plot(history.history['acc'],label=\"accuracy\")\n",
        "plt.plot(history.history['val_acc'],label=\"val_accuracy\")\n",
        "plt.plot(history.history['loss'],label=\"loss\")\n",
        "plt.plot(history.history['val_loss'],label=\"val_loss\")\n",
        "plt.legend()\n",
        "plt.title(\"FR Feature Selection\")\n",
        "plt.xlabel('epoch')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'epoch')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eLP8aEDWMIQ"
      },
      "source": [
        "## Save the model/weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T48k5-PWMIQ",
        "outputId": "f094f715-70aa-415d-e50b-f38655ad7c19"
      },
      "source": [
        "# JSON JSON\n",
        "# serialize model to json\n",
        "json_model = model.to_json()\n",
        "\n",
        "# save the model architecture to JSON file\n",
        "with open('{}/{}.model.json'.format(output_dir, model_name), 'w') as json_file:\n",
        "    json_file.write(json_model)\n",
        "\n",
        "\n",
        "# YAML YAML\n",
        "# serialize model to YAML\n",
        "model_yaml = model.to_yaml()\n",
        "\n",
        "# save the model architecture to YAML file\n",
        "with open(\"{}/{}.model.yaml\".format(output_dir, model_name), \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "\n",
        "\n",
        "# WEIGHTS HDF5\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"{}/{}.model.h5\".format(output_dir,model_name))\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujUx01jlWMIQ",
        "outputId": "5fbc657d-1112-4b2d-a2bd-f7e3cc1717a1"
      },
      "source": [
        "# Open the handle\n",
        "json_file = open('{}/{}.model.json'.format(output_dir, model_name), 'r')\n",
        "\n",
        "# load json and create model\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model.load_weights('{}/{}.model.h5'.format(output_dir, model_name))\n",
        "print(\"Loaded model from disk\")\n",
        "# loaded_model_json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnWNemmfWMIR",
        "outputId": "1e023db5-ef51-4a01-ff7e-9adca3af4ae5"
      },
      "source": [
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='categorical_crossentropy', optimizer='sgd', \n",
        "                     metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.22035759687423706\n",
            "Test accuracy: 0.9599999785423279\n",
            "accuracy: 96.00%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}